{
  "metadata" : {
    "id" : "edf204a5-5705-4330-ac35-28eb32ec99c0",
    "name" : "sensor-anomaly-detection-model-serving",
    "user_save_timestamp" : "1970-01-01T01:00:00.000Z",
    "auto_save_timestamp" : "1970-01-01T01:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "sparkNotebook" : null,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : [ "org.apache.spark %% spark-streaming-kafka-0-8 % 2.3.0", "org.apache.spark %% spark-sql-kafka-0-10 % 2.3.0" ],
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : {
      "spark.streaming.kafka.maxRatePerPartition" : "500",
      "jars" : ""
    },
    "customVars" : null
  },
  "cells" : [ {
    "metadata" : {
      "id" : "40AAFF23B9654D2A9B2E0C59515BAF8A"
    },
    "cell_type" : "markdown",
    "source" : "# Sensor Anomaly Model Serving"
  }, {
    "metadata" : {
      "id" : "D5BA242BD89A48F3816A0D7F53206849"
    },
    "cell_type" : "markdown",
    "source" : "In this notebook we quickly explore some specific aspects of the multi-level state management in Spark Streaming\n\nBy combining local and distributed state, we implement a simple sensor trend tracker that can help us identify and report anomalies."
  }, {
    "metadata" : {
      "id" : "79CD566650664F04A8515F55A809A44D"
    },
    "cell_type" : "markdown",
    "source" : "## Our Streaming dataset will consist of sensor information, containing the sensorId, a timestamp, and a value.\nThis component is a participant in a streaming pipeline.\n\nIt expects to receive moving averages of sensor data in the form of (id, timestamp, value) "
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "1BA7507DA513491F8D3A1AED38F087CB"
    },
    "cell_type" : "code",
    "source" : [ "import org.apache.spark.streaming.Seconds\n", "val topic = \"sensor-processed\"\n", "val kafkaBootstrapServer = \"172.17.0.2:9092\"\n", "val modelTopic = \"modelTopic\"\n", "val interval = Seconds(10)" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import org.apache.spark.streaming.Seconds\ntopic: String = sensor-processed\nkafkaBootstrapServer: String = 172.17.0.2:9092\nmodelTopic: String = modelTopic\ninterval: org.apache.spark.streaming.Duration = 10000 ms\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 1,
      "time" : "Took: 1.702s, at 2018-12-06 00:56"
    } ]
  }, {
    "metadata" : {
      "id" : "904E7DFFBF534F14832519CB2DF56D34"
    },
    "cell_type" : "markdown",
    "source" : "# Create a Streaming Standard Deviation Model\n## We can't use the classical model, as the data \"on the move\".\nThe classical formula for standard deviation : ![Standard Deviation Equation](https://wikimedia.org/api/rest_v1/media/math/render/svg/00eb0cde84f0a838a2de6db9f382866427aeb3bf) requires that all data is known beforehand.\n\n##  We need a specialized streaming algorithm\nTo obtain the standard deviation of a streaming dataset, we need an algorithm that provides an approximation of the `stdev` value.\n\nBased on https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance  we've chosen to implement [Welford's Online algorithm](https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Welford's_Online_algorithm)"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "C266DC9355EF4976A88F507075DCBE14"
    },
    "cell_type" : "code",
    "source" : [ "// State Definition for keeping the number of samples (n), the mean and the aggregated squared distance from the mean (m2)\n", "case class M2(n:Int, mean: Double, m2:Double) {\n", "  def variance: Option[Double] = {\n", "    if (n<2) None else Some(m2/(n-1))\n", "  }\n", "  def stdev: Option[Double] = variance.map(Math.sqrt)\n", "  }\n", "  object M2 extends Serializable {\n", "    val Zero = M2(0, 0.0, 0.0)\n", "  }" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "defined class M2\ndefined object M2\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 1,
      "time" : "Took: 1.358s, at 2018-12-06 01:04"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "B919847213E345A6834B0B35E0D9F9C0"
    },
    "cell_type" : "code",
    "source" : [ "// A collection to hold the state of all identified elements\n", "// this needs to be outside of the class b/c of Spark Notebook serialization\n", "var entries:Map[String, M2] = Map.empty" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "entries: Map[String,M2] = Map()\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 2,
      "time" : "Took: 1.261s, at 2018-12-06 01:05"
    } ]
  }, {
    "metadata" : {
      "id" : "66517DD418214B7686B3DA4C9C801ED6"
    },
    "cell_type" : "markdown",
    "source" : "## Welford's Algorithm Implemented on Spark Streaming\nWe use the low-level capabilities of Spark Streaming to implement a distributed computation of the updated values for\neach element in the incoming stream.\nWe combine that with a local in-memory structure to keep only the computed parameters for each entry.\n\nNote how computations on `RDD`s are executed in a distributed context. Only the results are collected back in the local context.\nWe define two methods: \n- `trainOn`: Receives a distributed collection and updates the model parameters for each id\n- `predictOnValues`: Estimates the mean and standard deviation for a given id."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "6C2DDCBFD1A24AE5AC93E34BEFC53FF8"
    },
    "cell_type" : "code",
    "source" : [ "import org.apache.spark.rdd.RDD\n", "import org.apache.spark.SparkContext\n", "import org.apache.spark.streaming.dstream.DStream\n", "  \n", "class M2Model() extends Serializable {\n", "  \n", "  def trainOn(dstream: DStream[(String, Double)]): Unit = {\n", "    dstream.foreachRDD{rdd => \n", "                       // This computation executes distributedly\n", "                       val newEntriesRDD = rdd.map{case (id, x) => \n", "                                                val current = entries.get(id)\n", "                                                val updated = current.map{case M2(n, mean, m2) => {\n", "                                                  val np = n + 1\n", "                                                  val delta = x - mean\n", "                                                  val meanp = mean + delta/np\n", "                                                  val mp2 = m2 + delta*(x - meanp)\n", "                                                  (id, M2(np, meanp, mp2))\n", "                                                  }\n", "                                                 }.getOrElse(id -> M2.Zero)\n", "                                                 updated\n", "                                               }\n", "                       // Here, we collect the results and update the local state\n", "                       val newEntries: Array[(String, M2)] = newEntriesRDD.collect\n", "                       entries = entries ++ newEntries\n", "                      }\n", "  }\n", "  \n", "  def predictOnValues(dstream: DStream[(String, Double)]): DStream[(String, Double, Double, Double)] = {\n", "    for { \n", "      (id, value) <- dstream\n", "      m2 <- entries.get(id)\n", "      stdev <- m2.stdev\n", "    } yield (id, value, m2.mean, stdev)\n", "  }\n", "}" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "<console>:54: warning: `withFilter' method does not yet exist on org.apache.spark.streaming.dstream.DStream[(String, Double)], using `filter' method instead\n             (id, value) <- dstream\n                            ^\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.SparkContext\nimport org.apache.spark.streaming.dstream.DStream\ndefined class M2Model\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 1,
      "time" : "Took: 1.491s, at 2018-12-06 01:13"
    } ]
  }, {
    "metadata" : {
      "id" : "BFF9E80BAB5C4D468154BCD6FB5C6501"
    },
    "cell_type" : "markdown",
    "source" : "## We create our Streaming Context"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "83B2B58C552240BBA6FF518B1AD274EB"
    },
    "cell_type" : "code",
    "source" : [ "import org.apache.spark.streaming.StreamingContext\n", "@transient val streamingContext = new StreamingContext(sparkSession.sparkContext, interval)" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import org.apache.spark.streaming.StreamingContext\nstreamingContext: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@1d81accd\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 2,
      "time" : "Took: 1.134s, at 2018-12-06 01:14"
    } ]
  }, {
    "metadata" : {
      "id" : "4E6DBF82BB6F4B6584233CD460F67263"
    },
    "cell_type" : "markdown",
    "source" : "## Our stream source will be a a Direct Kafka Stream\n"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "DF03F66BDDE0447B8202D39F2C0202E2"
    },
    "cell_type" : "code",
    "source" : [ "import org.apache.kafka.clients.consumer.ConsumerRecord\n", "import kafka.serializer.StringDecoder\n", "import org.apache.spark.streaming.kafka._\n", "\n", "val kafkaParams = Map[String, String](\n", "  \"metadata.broker.list\" -> kafkaBootstrapServer,\n", "  \"group.id\" -> \"sensor-tracker-group\",\n", "  \"auto.offset.reset\" -> \"largest\"\n", ")\n", "\n", "val topics = Set(topic)\n", "@transient val stream = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](\n", "     streamingContext, kafkaParams, topics)\n", "\n", "// kafka_010 APIs don't work on the Spark Notebook\n", "\n", "// @transient val stream = KafkaUtils.createDirectStream[String, String](\n", "//   streamingContext,\n", "//   PreferConsistent,\n", "//   Subscribe[String, String](topics, kafkaParams)\n", "// )\n", "\n" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "<console>:81: warning: object KafkaUtils in package kafka is deprecated: Update to Kafka 0.10 integration\n       @transient val stream = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](\n                               ^\nimport org.apache.kafka.clients.consumer.ConsumerRecord\nimport kafka.serializer.StringDecoder\nimport org.apache.spark.streaming.kafka._\nkafkaParams: scala.collection.immutable.Map[String,String] = Map(metadata.broker.list -> 172.17.0.2:9092, group.id -> sensor-tracker-group, auto.offset.reset -> largest)\ntopics: scala.collection.immutable.Set[String] = Set(sensor-processed)\nstream: org.apache.spark.streaming.dstream.InputDStream[(String, String)] = org.apache.spark.streaming.kafka.DirectKafkaInputDStream@60d44806\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 3,
      "time" : "Took: 1.763s, at 2018-12-06 01:14"
    } ]
  }, {
    "metadata" : {
      "id" : "CCCB597031E7451FB59D18BA85C0E4A4"
    },
    "cell_type" : "markdown",
    "source" : "# Providing Schema information for our streaming data\nNow that we have a DStream of fresh data processed in a 2-second interval, we can start focusing on the gist of this example.\nFirst, we want to define and apply a schema to the data we are receiving.\nIn Scala, we can define a schema with a `case class`"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "E7A917C393654969812E6E38223BBA52"
    },
    "cell_type" : "code",
    "source" : [ "case class SensorData(id: String, timestamp: Long, value: Double)" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "defined class SensorData\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 4,
      "time" : "Took: 0.824s, at 2018-12-06 01:14"
    } ]
  }, {
    "metadata" : {
      "id" : "CDF72D66AE6641E8B6D1D41BEAA87484"
    },
    "cell_type" : "markdown",
    "source" : "# Instantiate our Model\nWe will train an online standard deviation algorithm and use it to score the incoming data."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "B8733A11F3EF42A595D176FF91F39D4A"
    },
    "cell_type" : "code",
    "source" : [ "val model = new M2Model()" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "model: M2Model = M2Model@29e35041\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 5,
      "time" : "Took: 0.887s, at 2018-12-06 01:14"
    } ]
  }, {
    "metadata" : {
      "id" : "C5F61611DF7D431FA4208A02C738E4AF"
    },
    "cell_type" : "markdown",
    "source" : "# Convert the incoming JSON to `SensorData`\nSee how we interop with SparkSQL from Spark Streaming to use the JSON parsing facilities."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "0141D8A763F14F3A89A8AB0BCA0ADB5B"
    },
    "cell_type" : "code",
    "source" : [ "val spark = sparkSession\n", "import spark.implicits._\n", "@transient val sensorDataStream = stream.transform{rdd => \n", "                                        val jsonData = rdd.map{case (k,v)  => v}\n", "                                        val ds = sparkSession.createDataset(jsonData)\n", "                                        val jsonDF = spark.read.json(ds)\n", "                                        val sensorDataDS = jsonDF.as[SensorData]\n", "                                        sensorDataDS.rdd\n", "                                       }" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@44287f49\nimport spark.implicits._\nsensorDataStream: org.apache.spark.streaming.dstream.DStream[SensorData] = org.apache.spark.streaming.dstream.TransformedDStream@1dd636f5\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 6,
      "time" : "Took: 2.842s, at 2018-12-06 01:14"
    } ]
  }, {
    "metadata" : {
      "id" : "E28C393614DC43AC98FB04E6E4C23E32"
    },
    "cell_type" : "markdown",
    "source" : "## Prepare the data to train our model\nWe require the Id and the value that we want to use in our online standard deviation"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "A7350DE28CF14F72862E393EFBFF0885"
    },
    "cell_type" : "code",
    "source" : [ "@transient val inputData = sensorDataStream.transform {sensorDataRDD =>  \n", "                                                       sensorDataRDD.map{case SensorData(id,ts,value) => (id, value)}}                                                            " ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "inputData: org.apache.spark.streaming.dstream.DStream[(String, Double)] = org.apache.spark.streaming.dstream.TransformedDStream@7ea5b695\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 1,
      "time" : "Took: 1.616s, at 2018-12-06 01:16"
    } ]
  }, {
    "metadata" : {
      "id" : "B3467959C76443D8835F5FD18CD5F232"
    },
    "cell_type" : "markdown",
    "source" : "## Use the data to train the model"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "4439737C940E49FDACC1678A336BF926"
    },
    "cell_type" : "code",
    "source" : [ "model.trainOn(inputData)" ],
    "outputs" : [ {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 2,
      "time" : "Took: 1.788s, at 2018-12-06 01:16"
    } ]
  }, {
    "metadata" : {
      "id" : "E9B2777558F2465D829248EBC0AEE0FB"
    },
    "cell_type" : "markdown",
    "source" : "## Score the streaming data using the trained model"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "A36AA2E7377C4E15919C5A6C7B947B88"
    },
    "cell_type" : "code",
    "source" : [ "@transient val scored = model.predictOnValues(inputData)" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "scored: org.apache.spark.streaming.dstream.DStream[(String, Double, Double, Double)] = org.apache.spark.streaming.dstream.FlatMappedDStream@10826d8b\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 3,
      "time" : "Took: 1.172s, at 2018-12-06 01:16"
    } ]
  }, {
    "metadata" : {
      "id" : "6DD540344C9A417E8EF6072025E0A7D3"
    },
    "cell_type" : "markdown",
    "source" : "### Visualize the relation between the values and their standard deviation"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "DF831BD248614BC89C650598F9B24F3A"
    },
    "cell_type" : "code",
    "source" : [ "val scatterChart = new ScatterChart(Seq((0.0,0.0)))" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "scatterChart: notebook.front.widgets.charts.ScatterChart[Seq[(Double, Double)]] = <ScatterChart widget>\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 4,
      "time" : "Took: 1.284s, at 2018-12-06 01:17"
    } ]
  }, {
    "metadata" : {
      "id" : "A9F70A11C15C426CAC61DE78C83C830C"
    },
    "cell_type" : "markdown",
    "source" : "### Ouput Operations give us access to the data"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "489F976ACDEA435D874E144E9E9F00E2"
    },
    "cell_type" : "code",
    "source" : [ "scored.foreachRDD{rdd =>\n", "  val data = rdd.collect.map{case (id, value, mean, std) => (value, std)}.take(200)\n", "  scatterChart.applyOn(data)\n", "}" ],
    "outputs" : [ {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 5,
      "time" : "Took: 1.246s, at 2018-12-06 01:17"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "3F480D3477C542B691B114D7E92BFCEC"
    },
    "cell_type" : "code",
    "source" : [ "// Show the chart\n", "scatterChart" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res17: notebook.front.widgets.charts.ScatterChart[Seq[(Double, Double)]] = <ScatterChart widget>\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "<div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anonfc376eead2592ac990b735e3a8bb70d0&quot;,&quot;dataInit&quot;:[{&quot;_1&quot;:0,&quot;_2&quot;:0}],&quot;genId&quot;:&quot;953985999&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/scatterChart'], \n      function(playground, _magicscatterChart) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magicscatterChart,\n    \"o\": {\"x\":\"_1\",\"y\":\"_2\",\"width\":600,\"height\":400}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    <div>\n      <span class=\"chart-total-item-count\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anonb8674ca805d9a082176e98626634dcb4&quot;,&quot;initialValue&quot;:&quot;1&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p> entries total</span>\n      <span class=\"chart-sampling-warning\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anon8669dd8477f9d81b6d3daf1011e2fa73&quot;,&quot;initialValue&quot;:&quot;&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p></span>\n      <div>\n      </div>\n    </div></div>"
      },
      "output_type" : "execute_result",
      "execution_count" : 6,
      "time" : "Took: 1.194s, at 2018-12-06 01:17"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "3CE9860422D6489586334C3BC410879D"
    },
    "cell_type" : "code",
    "source" : [ "// Declare UI Widgets to see the data\n", "val debugBox = ul(15)\n", "debugBox.append(\"---\")" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "debugBox: notebook.front.widgets.HtmlList = <HtmlList widget>\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 7,
      "time" : "Took: 1.212s, at 2018-12-06 01:17"
    } ]
  }, {
    "metadata" : {
      "id" : "DBD0682F27A846B4BFAD453DFA7A4F4D"
    },
    "cell_type" : "markdown",
    "source" : "### Stream Content log (for mental sanity purposes)"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "B7690B3120FD4A2193BC5800B37C4F9D"
    },
    "cell_type" : "code",
    "source" : [ "debugBox" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res21: notebook.front.widgets.HtmlList = <HtmlList widget>\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "<ul data-bind=\"foreach: value\"><li data-bind=\"html: $data\"></li><script data-this=\"{&quot;valueId&quot;:&quot;anon47eca1d0fa89321e71c07676cc308c31&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId)\n    },\n    this\n  );\n});\n                            /*]]>*/</script></ul>"
      },
      "output_type" : "execute_result",
      "execution_count" : 8,
      "time" : "Took: 1.152s, at 2018-12-06 01:18"
    } ]
  }, {
    "metadata" : {
      "id" : "4DFDFFD45AAF499F99D2F5405DED9F8B"
    },
    "cell_type" : "markdown",
    "source" : "## `foreachRDD` Stream Output\nThe output operation lets us materialize the results.\n\nIn this notebook, we are going to output the results to the UI widgets we declared earlier."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "D691396F838E4087971FFDBFB3E06B7D"
    },
    "cell_type" : "code",
    "source" : [ "inputData.foreachRDD{rdd => \n", "                    val sample = rdd.take(20).map(_.toString)\n", "                    debugBox.appendAll(sample)\n", "                   } " ],
    "outputs" : [ {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 9,
      "time" : "Took: 1.216s, at 2018-12-06 01:18"
    } ]
  }, {
    "metadata" : {
      "id" : "D9E753EDE81F48B286C9D4CDA78C249A"
    },
    "cell_type" : "markdown",
    "source" : "## Write the trained model to the messaging backend"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "788349A5EFF84A448ED9928E8151D71B"
    },
    "cell_type" : "code",
    "source" : [ "case class IdM2(id:String, m2: M2)" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "defined class IdM2\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 10,
      "time" : "Took: 0.852s, at 2018-12-06 01:19"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "978CF682407245C6863A05CC5DAFB515"
    },
    "cell_type" : "code",
    "source" : [ "val spark = sparkSession\n", "import spark.implicits._\n", "import org.apache.spark.sql.types.StringType\n", "\n", "inputData.window(Seconds(30)).foreachRDD{ (rdd,time) => \n", "                                         if (!rdd.isEmpty) {\n", "                                           val modelDF = entries.map{case (id, m2) => IdM2(id, m2)}.toSeq.toDF\n", "                                           .select(to_json(struct(\"*\")).as(\"value\"))\n", "                                           .withColumn(\"key\", lit(null).cast(StringType))\n", "                                           modelDF.write.format(\"kafka\")\n", "                                           .option(\"kafka.bootstrap.servers\", kafkaBootstrapServer)\n", "                                            .option(\"topic\", modelTopic)\n", "                                           .save()\n", "                                         }\n", "                                        }" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@44287f49\nimport spark.implicits._\nimport org.apache.spark.sql.types.StringType\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 11,
      "time" : "Took: 2.260s, at 2018-12-06 01:19"
    } ]
  }, {
    "metadata" : {
      "id" : "9A8948CEE7B0493FA296C28F430452B2"
    },
    "cell_type" : "markdown",
    "source" : "## Starting the Context  initiates the stream processing"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "F366201F2275412F818532AB671A55BC"
    },
    "cell_type" : "code",
    "source" : [ "streamingContext.start()" ],
    "outputs" : [ {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 12,
      "time" : "Took: 1.237s, at 2018-12-06 01:19"
    } ]
  }, {
    "metadata" : {
      "id" : "2BF031E4D00D4CECB51B9F6412FDD97F"
    },
    "cell_type" : "markdown",
    "source" : "## `stop` destroys the streamingContext and stops the streaming computation  "
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "B6F0075E9BB04467858CABAA000489EF"
    },
    "cell_type" : "code",
    "source" : [ "// Be careful not to stop the context if you want the streaming process to continue\n", "streamingContext.stop(false)" ],
    "outputs" : [ {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 1,
      "time" : "Took: 1.371s, at 2018-10-27 05:45"
    } ]
  }, {
    "metadata" : {
      "id" : "6092D1EC1E3249BF84C4C9925A5AE29D"
    },
    "cell_type" : "markdown",
    "source" : "### We can 'snoop' in the values of our model. \nThe values are local to this process. Only computing them is done distributedly. "
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab1364664082-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "AFBE5F4FE7384EC6A5E4448760031620"
    },
    "cell_type" : "code",
    "source" : [ "entries(\"office\").stdev" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res30: Option[Double] = Some(10.835718130348406)\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "Some(10.835718130348406)"
      },
      "output_type" : "execute_result",
      "execution_count" : 13,
      "time" : "Took: 1.031s, at 2018-12-06 01:22"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "4ACD1B85C95046928D5D79E5321E5718"
    },
    "cell_type" : "code",
    "source" : [ "entries(\"office\").mean" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res32: Double = 36.38722109533468\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "36.38722109533468"
      },
      "output_type" : "execute_result",
      "execution_count" : 14,
      "time" : "Took: 1.124s, at 2018-12-06 01:22"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "EA2B7160FB97420BAFFDF259FE678EA9"
    },
    "cell_type" : "code",
    "source" : [ "entries(\"office\")" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res34: M2 = M2(18,38.15306513409961,3880.347227506937)\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "M2(18,38.15306513409961,3880.347227506937)"
      },
      "output_type" : "execute_result",
      "execution_count" : 15,
      "time" : "Took: 1.062s, at 2018-12-06 01:22"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "5458571C99D44C8B9DB0FC5E1C510863"
    },
    "cell_type" : "code",
    "source" : [ "\n" ],
    "outputs" : [ {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 36,
      "time" : "Took: 1.349s, at 2018-10-23 19:19"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "04915654A3524818BB7D1C982090C452"
    },
    "cell_type" : "code",
    "source" : [ "" ],
    "outputs" : [ ]
  } ],
  "nbformat" : 4
}