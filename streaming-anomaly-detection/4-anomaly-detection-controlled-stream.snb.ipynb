{
  "metadata" : {
    "id" : "2fd705e5-d6c7-4ba0-8142-39adf89e8596",
    "name" : "4-anomaly-detection-controlled-stream.snb.ipynb",
    "user_save_timestamp" : "2018-12-06T16:07:41.027Z",
    "auto_save_timestamp" : "1969-12-31T19:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "sparkNotebook" : null,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : [ "org.apache.spark %% spark-sql-kafka-0-10 % 2.3.0", "org.apache.spark %% spark-streaming-kafka-0-8 % 2.3.0" ],
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : {
      "spark.sql.codegen.wholeStage" : "false"
    },
    "customVars" : null
  },
  "cells" : [ {
    "metadata" : {
      "id" : "CE89CE47AA7D417581D75EA6E34A6CFD"
    },
    "cell_type" : "markdown",
    "source" : "#Anomaly Detection Using Controlled Streams\nThis notebook uses the exported M2 Model by Spark Streaming and combines it with Structured Streaming to deliver low-latency anomaly detection on the raw data stream."
  }, {
    "metadata" : {
      "id" : "92D6258DD425441388775107922BAF05"
    },
    "cell_type" : "markdown",
    "source" : "## Common settings"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "7868577A32F048309DE0AD504C46624C"
    },
    "cell_type" : "code",
    "source" : [ "import org.apache.spark.streaming.Seconds\n", "\n", "val topic = \"sensor-raw\"\n", "val modelTopic = \"modelTopic\"\n", "val anomalyTopic = \"anomalyTopic\"\n", "val kafkaBootstrapServer = \"172.17.0.2:9092\"\n", "val threshold = 4.0 // 5% failure rate\n", "val modelRefreshInterval = Seconds(30)" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import org.apache.spark.streaming.Seconds\ntopic: String = sensor-raw\nmodelTopic: String = modelTopic\nanomalyTopic: String = anomalyTopic\nkafkaBootstrapServer: String = 172.17.0.2:9092\nthreshold: Double = 4.0\nmodelRefreshInterval: org.apache.spark.streaming.Duration = 30000 ms\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 1,
      "time" : "Took: 1.636s, at 2018-12-06 15:40"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "9637A7E366B64271959BDF1A3FD093B3"
    },
    "cell_type" : "code",
    "source" : [ ":sh rm -rf /tmp/spark/detection/checkpoint" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "\nimport sys.process._\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/plain" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 2,
      "time" : "Took: 2.482s, at 2018-12-06 15:40"
    } ]
  }, {
    "metadata" : {
      "id" : "9391BBE443D0446E89FCA4FA1E27B6B5"
    },
    "cell_type" : "markdown",
    "source" : "## Case class and Schema definitions\n(we have seen this schema definition already in [sensor-anomaly-detection-model](./sensor-anomaly-detection-model-serving.snb.ipynb))"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "4E05EEA1D7A742DA9DABBF7832C645AB"
    },
    "cell_type" : "code",
    "source" : [ "case class M2(n:Int, mean: Double, m2:Double) {\n", "  def variance: Option[Double] = {\n", "    if (n<2) None else Some(m2/(n-1))\n", "  }\n", "  def stdev: Option[Double] = variance.map(Math.sqrt)\n", "}\n", "case class IdM2(id:String, m2: M2)\n", "case class SensorData(id: String, ts: Long, value: Double)" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "defined class M2\ndefined class IdM2\ndefined class SensorData\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 3,
      "time" : "Took: 1.369s, at 2018-12-06 15:40"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "31FBD06486E649468601F19E237E1375"
    },
    "cell_type" : "code",
    "source" : [ "import org.apache.spark.sql.Encoders\n", "val idM2Schema = Encoders.product[IdM2].schema\n", "val sensorSchema = Encoders.product[SensorData].schema" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import org.apache.spark.sql.Encoders\nidM2Schema: org.apache.spark.sql.types.StructType = StructType(StructField(id,StringType,true), StructField(m2,StructType(StructField(n,IntegerType,false), StructField(mean,DoubleType,false), StructField(m2,DoubleType,false)),true))\nsensorSchema: org.apache.spark.sql.types.StructType = StructType(StructField(id,StringType,true), StructField(ts,LongType,false), StructField(value,DoubleType,false))\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 4,
      "time" : "Took: 2.259s, at 2018-12-06 15:40"
    } ]
  }, {
    "metadata" : {
      "id" : "D31AABFB4BA644A2A52632A97B95DCB9"
    },
    "cell_type" : "markdown",
    "source" : "## Read the model Stream using Spark Streaming"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "70BB9604C6214CE68086EF1BCED2A368"
    },
    "cell_type" : "code",
    "source" : [ "import org.apache.spark.streaming.StreamingContext\n", "@transient val streamingContext = new StreamingContext(sparkSession.sparkContext, modelRefreshInterval)" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import org.apache.spark.streaming.StreamingContext\nstreamingContext: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@3095bc91\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 5,
      "time" : "Took: 1.087s, at 2018-12-06 15:41"
    } ]
  }, {
    "metadata" : {
      "id" : "F9F851C98F2849B18A8BFB05BCA47952"
    },
    "cell_type" : "markdown",
    "source" : "## Spark Streaming Kafka Source"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "6B9F8A8A791E40B58418DDCCE8FF2468"
    },
    "cell_type" : "code",
    "source" : [ "import org.apache.kafka.clients.consumer.ConsumerRecord\n", "import kafka.serializer.StringDecoder\n", "import org.apache.spark.streaming.kafka._\n", "\n", "val kafkaParams = Map[String, String](\n", "  \"metadata.broker.list\" -> kafkaBootstrapServer,\n", "  \"group.id\" -> \"model-serving-group\",\n", "  \"auto.offset.reset\" -> \"largest\"\n", ")\n", "\n", "val topics = Set(modelTopic)\n", "// There's a deprecation warning here. The Spark Notebook doesn't support Kafka 0.10 integration yet.\n", "@transient val modelStream = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](\n", "     streamingContext, kafkaParams, topics)\n" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "<console>:83: warning: object KafkaUtils in package kafka is deprecated: Update to Kafka 0.10 integration\n       @transient val modelStream = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](\n                                    ^\nimport org.apache.kafka.clients.consumer.ConsumerRecord\nimport kafka.serializer.StringDecoder\nimport org.apache.spark.streaming.kafka._\nkafkaParams: scala.collection.immutable.Map[String,String] = Map(metadata.broker.list -> 172.17.0.2:9092, group.id -> model-serving-group, auto.offset.reset -> largest)\ntopics: scala.collection.immutable.Set[String] = Set(modelTopic)\nmodelStream: org.apache.spark.streaming.dstream.InputDStream[(String, String)] = org.apache.spark.streaming.kafka.DirectKafkaInputDStream@5fa0d032\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 6,
      "time" : "Took: 2.071s, at 2018-12-06 15:41"
    } ]
  }, {
    "metadata" : {
      "id" : "9659B9FA5DA843AAAD40BA77B3D6919A"
    },
    "cell_type" : "markdown",
    "source" : "## Structured Streaming Kafka Source"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "62D6754FCB7B4AC48FC6595BB71A6C9F"
    },
    "cell_type" : "code",
    "source" : [ "val rawData = sparkSession.readStream\n", "  .format(\"kafka\")\n", "  .option(\"kafka.bootstrap.servers\", kafkaBootstrapServer)\n", "  .option(\"subscribe\", topic)\n", "  .option(\"startingOffsets\", \"latest\")\n", "  .load()\n", "val rawValues = rawData.selectExpr(\"CAST(value AS STRING)\").as[String]\n", "val jsonValues = rawValues.select(from_json($\"value\", sensorSchema) as \"record\")\n", "val sensorData = jsonValues.select(\"record.*\").as[SensorData]" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "rawData: org.apache.spark.sql.DataFrame = [key: binary, value: binary ... 5 more fields]\nrawValues: org.apache.spark.sql.Dataset[String] = [value: string]\njsonValues: org.apache.spark.sql.DataFrame = [record: struct<id: string, ts: bigint ... 1 more field>]\nsensorData: org.apache.spark.sql.Dataset[SensorData] = [id: string, ts: bigint ... 1 more field]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 7,
      "time" : "Took: 3.505s, at 2018-12-06 15:41"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "0B80CCD060AE42E78F4DC47572CB9317"
    },
    "cell_type" : "code",
    "source" : [ "var query: org.apache.spark.sql.streaming.StreamingQuery = _" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "query: org.apache.spark.sql.streaming.StreamingQuery = null\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 8,
      "time" : "Took: 0.933s, at 2018-12-06 15:41"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "EB0864F7A8394247B5EA0E1A7BE781E7"
    },
    "cell_type" : "code",
    "source" : [ "@transient val modelBox = ul(10)\n" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "modelBox: notebook.front.widgets.HtmlList = <HtmlList widget>\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 9,
      "time" : "Took: 1.171s, at 2018-12-06 15:41"
    } ]
  }, {
    "metadata" : {
      "id" : "3BE1959DCB9E4F5B861EA74F5B31DBD1"
    },
    "cell_type" : "markdown",
    "source" : "## Start Structured Streaming  Processing using Spark Streaming as Stream Flow\n"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "255A91A2EA3249B0B69D5CC7AD632818"
    },
    "cell_type" : "code",
    "source" : [ "import org.apache.spark.sql.functions._\n", "\n", "// ****************** Spark Streaming *********************\n", "modelStream.foreachRDD{ rdd =>\n", "  if (!rdd.isEmpty) {\n", "    \n", "    // ****************** Spark Streaming *********************\n", "    \n", "    // Extract the new model parameters received through Kafka\n", "    val models = rdd.map{case (k,v) => v}.toDF(\"value\")\n", "    val mostRecentM2JsonModel = models.select(from_json($\"value\", idM2Schema) as \"record\")\n", "    val mostRecentM2Model = mostRecentM2JsonModel.select(\"record.*\").as[IdM2]\n", "    val m2Map = mostRecentM2Model.collect.map(idM2=> (idM2.id, idM2.m2)).toMap\n", "    modelBox.appendAll(mostRecentM2Model.take(10).map(_.toString))\n", "    \n", "    \n", "    // ****************** Structured Streaming *****************\n", "    \n", "    // create a stream of scored data\n", "    val scoreStream = sensorData.flatMap{case SensorData(id, ts, value) => \n", "                                     val m2Opt = m2Map.get(id)\n", "                                     m2Opt.map{m2 => (id, ts, value, m2.mean, m2.stdev)}\n", "                                    }.toDF(\"id\", \"ts\",\"value\",\"mean\",\"stdev\")\n", "    \n", "    // filter suspected anomalies\n", "    val anomalies = scoreStream.where($\"value\" > ($\"mean\"+$\"stdev\"*threshold))\n", "                               .select($\"id\" as \"key\", to_json(struct($\"id\",$\"ts\", $\"value\", $\"stdev\")) as \"value\" )\n", "    \n", "    if (query != null) {\n", "      query.stop()\n", "    }\n", "    \n", "    // write the data back to Kafka\n", "    query = anomalies.writeStream\n", "      .format(\"kafka\")\n", "      .queryName(\"anomalyStreamProducer\")\n", "      //.trigger(Trigger.Continuous(\"10 second\"))\n", "      .outputMode(\"append\") \n", "      .option(\"kafka.bootstrap.servers\", kafkaBootstrapServer)\n", "      .option(\"topic\", anomalyTopic)\n", "      .option(\"checkpointLocation\", \"/tmp/spark/detection/checkpoint\")\n", "      .option(\"failOnDataLoss\", \"false\")\n", "    .start()\n", "    \n", "  }\n", "}\n", "\n", "    " ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import org.apache.spark.sql.functions._\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 10,
      "time" : "Took: 2.604s, at 2018-12-06 15:42"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "D329053A96304C78B525007B9F6D5456"
    },
    "cell_type" : "code",
    "source" : [ "streamingContext.start()" ],
    "outputs" : [ {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 11,
      "time" : "Took: 1.657s, at 2018-12-06 15:42"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "033E5123A5BE4C1491F8A3FEBCAFA850"
    },
    "cell_type" : "code",
    "source" : [ "// Data outout widget\n", "modelBox" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res15: notebook.front.widgets.HtmlList = <HtmlList widget>\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "<ul data-bind=\"foreach: value\"><li data-bind=\"html: $data\"></li><script data-this=\"{&quot;valueId&quot;:&quot;anon51d6fb6c379a393bc690d1ac52b7525f&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId)\n    },\n    this\n  );\n});\n                            /*]]>*/</script></ul>"
      },
      "output_type" : "execute_result",
      "execution_count" : 12,
      "time" : "Took: 1.340s, at 2018-12-06 15:42"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "B955BA0A45FF402E9F653A91DC857AEA"
    },
    "cell_type" : "code",
    "source" : [ "// streamingContext.stop(false)" ],
    "outputs" : [ {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 19,
      "time" : "Took: 1.306s, at 2018-12-06 13:22"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab1044633975-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "0760FC615D104A358BBAAF67E4C323BE"
    },
    "cell_type" : "code",
    "source" : [ "// uncomment to debug progress\n", "query.lastProgress" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "java.lang.NullPointerException\n  ... 69 elided\n"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "8C79874D846F4EA8849475E6C16BA620"
    },
    "cell_type" : "code",
    "source" : [ "" ],
    "outputs" : [ ]
  } ],
  "nbformat" : 4
}