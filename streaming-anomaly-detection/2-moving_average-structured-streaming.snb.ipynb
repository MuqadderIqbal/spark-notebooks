{
  "metadata" : {
    "id" : "de16a6ff-a497-4b7c-9d7b-1a1e049353e3",
    "name" : "2-moving_average-structured-streaming.snb.ipynb",
    "user_save_timestamp" : "2018-12-06T16:06:34.847Z",
    "auto_save_timestamp" : "1970-01-01T01:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "sparkNotebook" : null,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : [ "org.apache.spark %% spark-sql-kafka-0-10 % 2.3.0" ],
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : null,
    "customVars" : null
  },
  "cells" : [ {
    "metadata" : {
      "id" : "BCF6B238E1AA4C2885502F317090764B"
    },
    "cell_type" : "markdown",
    "source" : "# Improve Data Quality\nOur current raw data, direct from sensors, contains a lot of noise.\nWe can remove the noise by creating a moving average of the data stream\n\nIn this Notebook, we explore the event-time capabilities of Structured Streaming.\nWe are going to see:\n- How to define a field as `timestamp`\n- The use of watermarks to control old and out-of-order data\n- How to use SQL operators with time-based aggregations "
  }, {
    "metadata" : {
      "id" : "02A6398AF81A42088D75D34E7F17F7E3"
    },
    "cell_type" : "markdown",
    "source" : "##Common Definitions\nWe define a series of parameters of our current environment"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "9F07615090254EA69EE1256BE2AA6992"
    },
    "cell_type" : "code",
    "source" : [ "val sourceTopic = \"sensor-raw\"\n", "val targetTopic = \"sensor-processed\"\n", "val kafkaBootstrapServer = \"172.17.0.2:9092\" // local\n", "// val kafkaBootstrapServer = \"10.2.2.191:1025\" // fast-data-ec2\n" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "sourceTopic: String = sensor-raw\ntargetTopic: String = sensor-processed\nkafkaBootstrapServer: String = 172.17.0.2:9092\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 1,
      "time" : "Took: 0.995s, at 2018-12-06 15:38"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "11423A24D05E404698F69133DE97BA67"
    },
    "cell_type" : "code",
    "source" : [ ":sh rm -rf /tmp/spark/checkpoint" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "\nimport sys.process._\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/plain" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 2,
      "time" : "Took: 4.152s, at 2018-12-06 15:38"
    } ]
  }, {
    "metadata" : {
      "id" : "7DCDD5D54CC64A5C86796F8F336D9D13"
    },
    "cell_type" : "markdown",
    "source" : "## Read the data stream from Kafka and Extract + Transform the Payload\nWe use the kafka source to subscribe to the `sourceTopic` that contains the raw sensor data.\nThis results in a streaming dataframe that we use to operate on the underlying data.\n\n_Tip: We saw already how to do this in:_ [Processing Sensor Data from Kafka with Structured Streaming](./raw_sensor_stream_Structured_Streaming.snb.ipynb)"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "1AA90821F2104B8EA69F591C80FA5C78"
    },
    "cell_type" : "code",
    "source" : [ "val rawData = sparkSession.readStream\n", "      .format(\"kafka\")\n", "      .option(\"kafka.bootstrap.servers\", kafkaBootstrapServer)\n", "      .option(\"subscribe\", sourceTopic)\n", "      .option(\"startingOffsets\", \"latest\")\n", "      .load()" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "rawData: org.apache.spark.sql.DataFrame = [key: binary, value: binary ... 5 more fields]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 3,
      "time" : "Took: 1.560s, at 2018-12-06 15:38"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "3CC81180F5E84F8289D8E851ED55C648"
    },
    "cell_type" : "code",
    "source" : [ "import org.apache.spark.sql.Encoders\n", "// Schema definition as case class\n", "case class SensorData(id: String, ts: Long, value: Double)\n", "// schema definition as SparkSQL struct\n", "val schema = Encoders.product[SensorData].schema" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import org.apache.spark.sql.Encoders\ndefined class SensorData\nschema: org.apache.spark.sql.types.StructType = StructType(StructField(id,StringType,true), StructField(ts,LongType,false), StructField(value,DoubleType,false))\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 4,
      "time" : "Took: 1.198s, at 2018-12-06 15:38"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "119B42A214EE4AAE8285287FFCE2F068"
    },
    "cell_type" : "code",
    "source" : [ "// Payload extraction from the `value` field as a `String`\n", "val rawValues = rawData.selectExpr(\"CAST(value AS STRING)\").as[String]\n", "// Parse the `String` data as a JSON object\n", "val jsonValues = rawValues.select(from_json($\"value\", schema) as \"record\")\n", "// Create a strongly-typed Dataset using the `case class` definition\n", "val sensorData = jsonValues.select(\"record.*\").as[SensorData]" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "rawValues: org.apache.spark.sql.Dataset[String] = [value: string]\njsonValues: org.apache.spark.sql.DataFrame = [record: struct<id: string, ts: bigint ... 1 more field>]\nsensorData: org.apache.spark.sql.Dataset[SensorData] = [id: string, ts: bigint ... 1 more field]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 5,
      "time" : "Took: 1.502s, at 2018-12-06 15:38"
    } ]
  }, {
    "metadata" : {
      "id" : "431380BA6B004FEF8D09F931E2248B88"
    },
    "cell_type" : "markdown",
    "source" : "# Reduce Noise in Data using a Moving Average\nWe use the sliding window function in Structured Streaming to implement a moving average over the data stream."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "17338E711F2047D38FC1EFAC165BB2D2"
    },
    "cell_type" : "code",
    "source" : [ "import org.apache.spark.sql.types._" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import org.apache.spark.sql.types._\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 6,
      "time" : "Took: 0.784s, at 2018-12-06 15:38"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "2C52DB59E7CC41AFA697260B0156C7D8"
    },
    "cell_type" : "code",
    "source" : [ "val toTimestamp = udf((ts:Long) => new java.sql.Timestamp(ts))\n", "val sensorMovingAverage = sensorData.withColumn(\"timestamp\", toTimestamp($\"ts\"))\n", "                                          .withWatermark(\"timestamp\", \"10 seconds\")\n", "                                          .groupBy($\"id\", window($\"timestamp\", \"30 seconds\", \"10 seconds\"))\n", "                                          .agg(avg($\"value\") as \"avg_value\")" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "toTimestamp: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,TimestampType,Some(List(LongType)))\nsensorMovingAverage: org.apache.spark.sql.DataFrame = [id: string, window: struct<start: timestamp, end: timestamp> ... 1 more field]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 7,
      "time" : "Took: 1.049s, at 2018-12-06 15:38"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "C58D8B90AD6E4F5F82BD91CAA3EEE8FC"
    },
    "cell_type" : "code",
    "source" : [ "sensorMovingAverage.printSchema" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "root\n |-- id: string (nullable = true)\n |-- window: struct (nullable = true)\n |    |-- start: timestamp (nullable = true)\n |    |-- end: timestamp (nullable = true)\n |-- avg_value: double (nullable = true)\n\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 8,
      "time" : "Took: 0.861s, at 2018-12-06 15:38"
    } ]
  }, {
    "metadata" : {
      "id" : "A278C59A15B74FFD8768F9DD8DECA53A"
    },
    "cell_type" : "markdown",
    "source" : "# Write our moving average data to our `sensor-clean` topic\nTo write data to Kafka, we need to transform our data into a `(key, value)` pair, where the `key` is used for partitioning in Kafka and the `value` contains our payload.\n\nWe will use JSON Encoding for our data structure."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "17968E2E0AA0415888F73DF26F7C97FF"
    },
    "cell_type" : "code",
    "source" : [ "// First we prepare the schema to comply with the (key, value) model of Kafka\n", "// don't be confused with the different `value` fields. One is from our data, the other is the Kafka payload\n", "val kafkaFormat = sensorMovingAverage\n", ".select($\"id\", $\"window.start\".cast(LongType) as \"ts\", $\"avg_value\" as \"value\")\n", ".select($\"id\" as \"key\", to_json(struct($\"id\", $\"ts\", $\"value\")) as \"value\")" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "kafkaFormat: org.apache.spark.sql.DataFrame = [key: string, value: string]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 9,
      "time" : "Took: 0.652s, at 2018-12-06 15:38"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "81238BD9A161498F83264B20BDE60F93"
    },
    "cell_type" : "code",
    "source" : [ "// We write to the `targetTopic`\n", "val kafkaWriterQuery = kafkaFormat.writeStream\n", "  .queryName(\"kafkaWriter\") \n", "  .outputMode(\"append\") \n", "  .format(\"kafka\")\n", "  .option(\"kafka.bootstrap.servers\", kafkaBootstrapServer)\n", "  .option(\"topic\", targetTopic)\n", "  .option(\"checkpointLocation\", \"/tmp/spark/checkpoint\")\n", "  .option(\"failOnDataLoss\", \"false\")\n", "  .start()" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "kafkaWriterQuery: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@15f405e6\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 10,
      "time" : "Took: 0.850s, at 2018-12-06 15:38"
    } ]
  }, {
    "metadata" : {
      "id" : "EA0295A5FE6D406C83FCB363EBD45E06"
    },
    "cell_type" : "markdown",
    "source" : "## View Progress"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "1A4A6A1BFB8D40E39ED0D253F5015D16"
    },
    "cell_type" : "code",
    "source" : [ "val progress = kafkaWriterQuery.recentProgress" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "progress: Array[org.apache.spark.sql.streaming.StreamingQueryProgress] =\nArray({\n  \"id\" : \"aa01fb27-4c64-4a6e-b22a-0fd9f04af0a3\",\n  \"runId\" : \"25554702-58f6-43f2-a13f-b805f1bf29af\",\n  \"name\" : \"kafkaWriter\",\n  \"timestamp\" : \"2018-12-06T14:38:24.948Z\",\n  \"batchId\" : 0,\n  \"numInputRows\" : 2,\n  \"processedRowsPerSecond\" : 0.3306878306878307,\n  \"durationMs\" : {\n    \"addBatch\" : 5150,\n    \"getBatch\" : 253,\n    \"getOffset\" : 276,\n    \"queryPlanning\" : 303,\n    \"triggerExecution\" : 6047,\n    \"walCommit\" : 38\n  },\n  \"eventTime\" : {\n    \"avg\" : \"2018-12-06T14:38:25.213Z\",\n    \"max\" : \"2018-12-06T14:38:25.228Z\",\n    \"min\" : \"2018-12-06T14:38:25.198Z\",\n    \"watermark\" : \"1970-01-01T00:00:00.000Z\"\n  },\n  \"stateOperators\" : [ {\n    \"numRowsTotal\" : 6,\n    \"numRowsUpdated\" : 6,\n    \"memoryUsedBytes\" :..."
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 12,
      "time" : "Took: 0.858s, at 2018-12-06 15:38"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "6252EAE9B2F54C3588BB0292C5AD0355"
    },
    "cell_type" : "code",
    "source" : [ "" ],
    "outputs" : [ ]
  } ],
  "nbformat" : 4
}