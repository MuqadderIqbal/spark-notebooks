{
  "metadata" : {
    "id" : "688315e2-be63-45c3-b63e-0a92874629df",
    "name" : "sensor-anomaly-detection-model",
    "user_save_timestamp" : "1970-01-01T01:00:00.000Z",
    "auto_save_timestamp" : "1970-01-01T01:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "sparkNotebook" : null,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : [ "org.apache.spark %% spark-streaming-kafka-0-8 % 2.1.0" ],
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : {
      "spark.streaming.kafka.maxRatePerPartition" : "500",
      "jars" : ""
    },
    "customVars" : null
  },
  "cells" : [ {
    "metadata" : {
      "id" : "40AAFF23B9654D2A9B2E0C59515BAF8A"
    },
    "cell_type" : "markdown",
    "source" : "# Sensor Anomaly Detection"
  }, {
    "metadata" : {
      "id" : "D5BA242BD89A48F3816A0D7F53206849"
    },
    "cell_type" : "markdown",
    "source" : "In this notebook we quickly explore some specific aspects of multi-level state management in Spark Streaming\n\nBy combining local and distributed state, we implement a simple sensor trend tracker that can help us identify and report anomalies."
  }, {
    "metadata" : {
      "id" : "79CD566650664F04A8515F55A809A44D"
    },
    "cell_type" : "markdown",
    "source" : "## Our Streaming dataset will consist of sensor information, containing the sensorId, a timestamp, and a value.\nThis component is a participant in a streaming pipeline.\n\nIt expects to receive moving averages of sensor data in the form of (id, timestamp, value) "
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "1BA7507DA513491F8D3A1AED38F087CB"
    },
    "cell_type" : "code",
    "source" : [ "import org.apache.spark.streaming.Seconds\n", "val topic = \"sensor-processed\"\n", "val kafkaBootstrapServer = \"172.17.0.2:9092\"\n", "val threshold = 4.0\n", "val interval = Seconds(10)" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import org.apache.spark.streaming.Seconds\ntopic: String = sensor-processed\nkafkaBootstrapServer: String = 172.17.0.2:9092\nthreshold: Double = 4.0\ninterval: org.apache.spark.streaming.Duration = 10000 ms\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 2,
      "time" : "Took: 1.621s, at 2018-09-11 13:08"
    } ]
  }, {
    "metadata" : {
      "id" : "904E7DFFBF534F14832519CB2DF56D34"
    },
    "cell_type" : "markdown",
    "source" : "# Create a Streaming Standard Deviation Model\nBased on https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "C266DC9355EF4976A88F507075DCBE14"
    },
    "cell_type" : "code",
    "source" : [ "case class M2(n:Int, mean: Double, m2:Double) {\n", "  def variance: Option[Double] = {\n", "    if (n<2) None else Some(m2/(n-1))\n", "  }\n", "  def stdev: Option[Double] = variance.map(Math.sqrt)\n", "  }\n", "  object M2 extends Serializable {\n", "    val Zero = M2(0,0.0,0.0)\n", "  }" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "defined class M2\ndefined object M2\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 3,
      "time" : "Took: 1.392s, at 2018-09-11 13:08"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "B919847213E345A6834B0B35E0D9F9C0"
    },
    "cell_type" : "code",
    "source" : [ "// this needs to be outside of the class b/c of Spark Notebook serialization\n", "var entries:Map[String, M2] = Map.empty" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "entries: Map[String,M2] = Map()\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 4,
      "time" : "Took: 1.679s, at 2018-09-11 13:08"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "6C2DDCBFD1A24AE5AC93E34BEFC53FF8"
    },
    "cell_type" : "code",
    "source" : [ "import org.apache.spark.rdd.RDD\n", "import org.apache.spark.SparkContext\n", "import org.apache.spark.streaming.dstream.DStream\n", "  \n", "class M2Model() extends Serializable {\n", "  \n", "  def trainOn(dstream: DStream[(String, Double)]): Unit = {\n", "    dstream.foreachRDD{rdd => \n", "                       val newEntriesRDD = rdd.map{case (id, x) => \n", "                                                val current = entries.get(id)\n", "                                                val updated = current.map{case M2(n, mean, m2) => {\n", "                                                  val np = n + 1\n", "                                                  val delta = x - mean\n", "                                                  val meanp = mean + delta/np\n", "                                                  val mp2 = m2 + delta*(x - meanp)\n", "                                                  (id, M2(np, meanp, mp2))\n", "                                                  }\n", "                                                 }.getOrElse(id -> M2.Zero)\n", "                                                 updated\n", "                                               }\n", "                       val newEntries: Array[(String, M2)] = newEntriesRDD.collect\n", "                       entries = entries ++ newEntries\n", "                      }\n", "  }\n", "  def predictOnValues(dstream: DStream[(String, Double)]): DStream[(String, Double, Double, Double)] = {\n", "    for { \n", "      (id, value) <- dstream\n", "      m2 <- entries.get(id)\n", "      stdev <- m2.stdev\n", "    } yield (id, value, m2.mean, stdev)\n", "  }\n", "}" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "<console>:52: warning: `withFilter' method does not yet exist on org.apache.spark.streaming.dstream.DStream[(String, Double)], using `filter' method instead\n             (id, value) <- dstream\n                            ^\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.SparkContext\nimport org.apache.spark.streaming.dstream.DStream\ndefined class M2Model\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 5,
      "time" : "Took: 1.501s, at 2018-09-11 13:08"
    } ]
  }, {
    "metadata" : {
      "id" : "BFF9E80BAB5C4D468154BCD6FB5C6501"
    },
    "cell_type" : "markdown",
    "source" : "## We create our Streaming Context"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "83B2B58C552240BBA6FF518B1AD274EB"
    },
    "cell_type" : "code",
    "source" : [ "import org.apache.spark.streaming.StreamingContext\n", "@transient val streamingContext = new StreamingContext(sparkSession.sparkContext, interval)" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import org.apache.spark.streaming.StreamingContext\nstreamingContext: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@3d384a05\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 7,
      "time" : "Took: 1.149s, at 2018-09-11 13:08"
    } ]
  }, {
    "metadata" : {
      "id" : "4E6DBF82BB6F4B6584233CD460F67263"
    },
    "cell_type" : "markdown",
    "source" : "## Our stream source will be a a Direct Kafka Stream\n"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "DF03F66BDDE0447B8202D39F2C0202E2"
    },
    "cell_type" : "code",
    "source" : [ "import org.apache.kafka.clients.consumer.ConsumerRecord\n", "import kafka.serializer.StringDecoder\n", "import org.apache.spark.streaming.kafka._\n", "\n", "val kafkaParams = Map[String, String](\n", "  \"metadata.broker.list\" -> kafkaBootstrapServer,\n", "  \"group.id\" -> \"sensor-tracker-group\",\n", "  \"auto.offset.reset\" -> \"largest\",\n", "  \"enable.auto.commit\" -> (false: java.lang.Boolean).toString\n", ")\n", "\n", "val topics = Set(topic)\n", "@transient val stream = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](\n", "     streamingContext, kafkaParams, topics)\n", "\n", "// kafka_010 APIs don't work on the Spark Notebook\n", "\n", "// @transient val stream = KafkaUtils.createDirectStream[String, String](\n", "//   streamingContext,\n", "//   PreferConsistent,\n", "//   Subscribe[String, String](topics, kafkaParams)\n", "// )\n", "\n" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import org.apache.kafka.clients.consumer.ConsumerRecord\nimport kafka.serializer.StringDecoder\nimport org.apache.spark.streaming.kafka._\nkafkaParams: scala.collection.immutable.Map[String,String] = Map(metadata.broker.list -> 172.17.0.2:9092, group.id -> sensor-tracker-group, auto.offset.reset -> largest, enable.auto.commit -> false)\ntopics: scala.collection.immutable.Set[String] = Set(sensor-processed)\nstream: org.apache.spark.streaming.dstream.InputDStream[(String, String)] = org.apache.spark.streaming.kafka.DirectKafkaInputDStream@4e823154\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 8,
      "time" : "Took: 2.510s, at 2018-09-11 13:09"
    } ]
  }, {
    "metadata" : {
      "id" : "CCCB597031E7451FB59D18BA85C0E4A4"
    },
    "cell_type" : "markdown",
    "source" : "# Providing Schema information for our streaming data\nNow that we have a DStream of fresh data processed in a 2-second interval, we can start focusing on the gist of this example.\nFirst, we want to define and apply a schema to the data we are receiving.\nIn Scala, we can define a schema with a `case class`"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "E7A917C393654969812E6E38223BBA52"
    },
    "cell_type" : "code",
    "source" : [ "case class SensorData(id: String, timestamp: Long, temp: Double)" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "defined class SensorData\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 9,
      "time" : "Took: 1.138s, at 2018-09-11 13:09"
    } ]
  }, {
    "metadata" : {
      "id" : "CDF72D66AE6641E8B6D1D41BEAA87484"
    },
    "cell_type" : "markdown",
    "source" : "# Create our Model\nWe will train an online standard deviation algorithm and use it to score the incoming data."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "B8733A11F3EF42A595D176FF91F39D4A"
    },
    "cell_type" : "code",
    "source" : [ "val model = new M2Model()" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "model: M2Model = M2Model@513654ec\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 10,
      "time" : "Took: 1.251s, at 2018-09-11 13:09"
    } ]
  }, {
    "metadata" : {
      "id" : "C5F61611DF7D431FA4208A02C738E4AF"
    },
    "cell_type" : "markdown",
    "source" : "# Convert the incoming JSON to `SensorData`\nSee how we interop with SparkSQL from Spark Streaming to use the JSON parsing facilities."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "0141D8A763F14F3A89A8AB0BCA0ADB5B"
    },
    "cell_type" : "code",
    "source" : [ "val spark = sparkSession\n", "import spark.implicits._\n", "@transient val sensorDataStream = stream.transform{rdd => \n", "                                        val jsonData = rdd.map{case (k,v)  => v}\n", "                                        val ds = sparkSession.createDataset(jsonData)\n", "                                        val jsonDF = spark.read.json(ds)\n", "                                        val sensorDataDS = jsonDF.as[SensorData]\n", "                                        sensorDataDS.rdd\n", "                                       }" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@654eade3\nimport spark.implicits._\nsensorDataStream: org.apache.spark.streaming.dstream.DStream[SensorData] = org.apache.spark.streaming.dstream.TransformedDStream@372792c4\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 11,
      "time" : "Took: 3.594s, at 2018-09-11 13:10"
    } ]
  }, {
    "metadata" : {
      "id" : "E28C393614DC43AC98FB04E6E4C23E32"
    },
    "cell_type" : "markdown",
    "source" : "# Prepare the stream to train our model"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "A7350DE28CF14F72862E393EFBFF0885"
    },
    "cell_type" : "code",
    "source" : [ "@transient val inputData = sensorDataStream.transform {sensorDataRDD =>  \n", "                                                       sensorDataRDD.map{case SensorData(id,ts,value) => (id, value)}}                                                            " ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "inputData: org.apache.spark.streaming.dstream.DStream[(String, Double)] = org.apache.spark.streaming.dstream.TransformedDStream@68db089b\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 12,
      "time" : "Took: 1.586s, at 2018-09-11 13:10"
    } ]
  }, {
    "metadata" : {
      "id" : "B3467959C76443D8835F5FD18CD5F232"
    },
    "cell_type" : "markdown",
    "source" : "## Use the data to train the model"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "4439737C940E49FDACC1678A336BF926"
    },
    "cell_type" : "code",
    "source" : [ "model.trainOn(inputData)" ],
    "outputs" : [ {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 13,
      "time" : "Took: 1.745s, at 2018-09-11 13:10"
    } ]
  }, {
    "metadata" : {
      "id" : "E9B2777558F2465D829248EBC0AEE0FB"
    },
    "cell_type" : "markdown",
    "source" : "## Score the streaming data using the trained model"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "A36AA2E7377C4E15919C5A6C7B947B88"
    },
    "cell_type" : "code",
    "source" : [ "@transient val scored = model.predictOnValues(inputData)" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "scored: org.apache.spark.streaming.dstream.DStream[(String, Double, Double, Double)] = org.apache.spark.streaming.dstream.FlatMappedDStream@75022df9\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 14,
      "time" : "Took: 1.476s, at 2018-09-11 13:11"
    } ]
  }, {
    "metadata" : {
      "id" : "6DD540344C9A417E8EF6072025E0A7D3"
    },
    "cell_type" : "markdown",
    "source" : "### Visualize the relation between the values and their standard deviation"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "DF831BD248614BC89C650598F9B24F3A"
    },
    "cell_type" : "code",
    "source" : [ "val scatterChart = new ScatterChart(Seq((0.0,0.0)))\n", "scatterChart\n" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "scatterChart: notebook.front.widgets.charts.ScatterChart[Seq[(Double, Double)]] = <ScatterChart widget>\nres19: notebook.front.widgets.charts.ScatterChart[Seq[(Double, Double)]] = <ScatterChart widget>\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "<div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anon682b290d922648543e17864ac3fe7f13&quot;,&quot;dataInit&quot;:[{&quot;_1&quot;:0,&quot;_2&quot;:0}],&quot;genId&quot;:&quot;1777618150&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/scatterChart'], \n      function(playground, _magicscatterChart) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magicscatterChart,\n    \"o\": {\"x\":\"_1\",\"y\":\"_2\",\"width\":600,\"height\":400}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    <div>\n      <span class=\"chart-total-item-count\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anon51aec9f28b4651873763c44eae1a01ee&quot;,&quot;initialValue&quot;:&quot;1&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p> entries total</span>\n      <span class=\"chart-sampling-warning\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anon178ea0eae199dd4f8350d1aaebdcc26e&quot;,&quot;initialValue&quot;:&quot;&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p></span>\n      <div>\n      </div>\n    </div></div>"
      },
      "output_type" : "execute_result",
      "execution_count" : 15,
      "time" : "Took: 2.222s, at 2018-09-11 13:11"
    } ]
  }, {
    "metadata" : {
      "id" : "A9F70A11C15C426CAC61DE78C83C830C"
    },
    "cell_type" : "markdown",
    "source" : "### Ouput Operations give us access to the data"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "489F976ACDEA435D874E144E9E9F00E2"
    },
    "cell_type" : "code",
    "source" : [ "scored.foreachRDD{rdd =>\n", "  val data = rdd.collect.map{case (id, value, mean, std) => (value, std)}\n", "  scatterChart.applyOn(data)\n", "}" ],
    "outputs" : [ {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 16,
      "time" : "Took: 1.763s, at 2018-09-11 13:12"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "3CE9860422D6489586334C3BC410879D"
    },
    "cell_type" : "code",
    "source" : [ "// Declare UI Widgets to see the data\n", "val outputBox = ul(20)\n", "outputBox.append(\"---\")\n", "val debugBox = ul(15)\n", "debugBox.append(\"---\")" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "outputBox: notebook.front.widgets.HtmlList = <HtmlList widget>\ndebugBox: notebook.front.widgets.HtmlList = <HtmlList widget>\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 17,
      "time" : "Took: 1.786s, at 2018-09-11 13:12"
    } ]
  }, {
    "metadata" : {
      "id" : "DE74ECC18C0E4E4C80659873348F1276"
    },
    "cell_type" : "markdown",
    "source" : "## Detected Anomalies"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "EA5A8CC609964B7488A090B21360C4F1"
    },
    "cell_type" : "code",
    "source" : [ "\n", "outputBox" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res25: notebook.front.widgets.HtmlList = <HtmlList widget>\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "<ul data-bind=\"foreach: value\"><li data-bind=\"html: $data\"></li><script data-this=\"{&quot;valueId&quot;:&quot;anonf829c3923ff64b6b0d58f7959f3da845&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId)\n    },\n    this\n  );\n});\n                            /*]]>*/</script></ul>"
      },
      "output_type" : "execute_result",
      "execution_count" : 18,
      "time" : "Took: 1.568s, at 2018-09-11 13:12"
    } ]
  }, {
    "metadata" : {
      "id" : "DBD0682F27A846B4BFAD453DFA7A4F4D"
    },
    "cell_type" : "markdown",
    "source" : "### Stream Content log (for sanity purposes)"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "B7690B3120FD4A2193BC5800B37C4F9D"
    },
    "cell_type" : "code",
    "source" : [ "debugBox" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res27: notebook.front.widgets.HtmlList = <HtmlList widget>\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "<ul data-bind=\"foreach: value\"><li data-bind=\"html: $data\"></li><script data-this=\"{&quot;valueId&quot;:&quot;anonb449c6f8d293591344d733823d359dc1&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId)\n    },\n    this\n  );\n});\n                            /*]]>*/</script></ul>"
      },
      "output_type" : "execute_result",
      "execution_count" : 19,
      "time" : "Took: 1.597s, at 2018-09-11 13:12"
    } ]
  }, {
    "metadata" : {
      "id" : "80A13555C3A5439E8DA4D7C7C6C781EC"
    },
    "cell_type" : "markdown",
    "source" : "## Anomaly Detection using Streaming Standard Deviation\nValues beyond the threshhold-times the standard deviation around the mean are considered irregular and deserve scrutiny  "
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "73F95E77619145E2BC3A4250D7734B96"
    },
    "cell_type" : "code",
    "source" : [ "@transient val suspects = scored.filter{case (id, value, mean, std) => \n", "                                        (value > mean + std * threshold) || (value < mean - std * threshold)\n", "                                       }" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "suspects: org.apache.spark.streaming.dstream.DStream[(String, Double, Double, Double)] = org.apache.spark.streaming.dstream.FilteredDStream@57c0e8a9\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 20,
      "time" : "Took: 1.755s, at 2018-09-11 13:13"
    } ]
  }, {
    "metadata" : {
      "id" : "4DFDFFD45AAF499F99D2F5405DED9F8B"
    },
    "cell_type" : "markdown",
    "source" : "## `foreachRDD` Stream Output\nThe output operation lets us materialize the results.\n\nIn this notebook, we are going to output the results to the UI widgets we declared earlier."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "9133EFDA4CAB4DAF8ACA0E6C753E2A9E"
    },
    "cell_type" : "code",
    "source" : [ "suspects.foreachRDD{rdd => \n", "                      val top20 = rdd.take(20).map(_.toString)\n", "                      val total = s\"total anomalies found: ${rdd.count}\"\n", "                      outputBox(total +: top20)\n", "                    }                  " ],
    "outputs" : [ {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 21,
      "time" : "Took: 1.882s, at 2018-09-11 13:13"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "D691396F838E4087971FFDBFB3E06B7D"
    },
    "cell_type" : "code",
    "source" : [ "inputData.foreachRDD{rdd => \n", "                    val sample = rdd.take(20).map(_.toString)\n", "                    debugBox.appendAll(sample)\n", "                   } " ],
    "outputs" : [ {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 22,
      "time" : "Took: 1.615s, at 2018-09-11 13:13"
    } ]
  }, {
    "metadata" : {
      "id" : "9A8948CEE7B0493FA296C28F430452B2"
    },
    "cell_type" : "markdown",
    "source" : "## Starting the Context  initiates the stream processing"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "F366201F2275412F818532AB671A55BC"
    },
    "cell_type" : "code",
    "source" : [ "streamingContext.start()" ],
    "outputs" : [ {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 23,
      "time" : "Took: 1.564s, at 2018-09-11 13:14"
    } ]
  }, {
    "metadata" : {
      "id" : "2BF031E4D00D4CECB51B9F6412FDD97F"
    },
    "cell_type" : "markdown",
    "source" : "## `stop` destroys the streamingContext and stops the streaming computation  "
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "B6F0075E9BB04467858CABAA000489EF"
    },
    "cell_type" : "code",
    "source" : [ "// Be careful not to stop the context if you want the streaming process to continue\n", "streamingContext.stop(false)" ],
    "outputs" : [ {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 19,
      "time" : "Took: 1.007s, at 2018-09-11 11:00"
    } ]
  }, {
    "metadata" : {
      "id" : "6092D1EC1E3249BF84C4C9925A5AE29D"
    },
    "cell_type" : "markdown",
    "source" : "### We can 'snoop' in the values of our model. \nThe values are local to this process. Only computing them is done distributedly. "
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab1364664082-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "AFBE5F4FE7384EC6A5E4448760031620"
    },
    "cell_type" : "code",
    "source" : [ "entries(\"office\").stdev" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res41: Option[Double] = Some(1.400086241511657)\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "Some(1.400086241511657)"
      },
      "output_type" : "execute_result",
      "execution_count" : 16,
      "time" : "Took: 0.894s, at 2018-09-11 10:54"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "EA2B7160FB97420BAFFDF259FE678EA9"
    },
    "cell_type" : "code",
    "source" : [ "entries(\"office\")" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res43: M2 = M2(31,20.454768088208496,56.85374395997885)\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "M2(31,20.454768088208496,56.85374395997885)"
      },
      "output_type" : "execute_result",
      "execution_count" : 17,
      "time" : "Took: 0.819s, at 2018-09-11 10:54"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "5458571C99D44C8B9DB0FC5E1C510863"
    },
    "cell_type" : "code",
    "source" : [ "" ],
    "outputs" : [ ]
  } ],
  "nbformat" : 4
}