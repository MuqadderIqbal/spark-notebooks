{
  "metadata" : {
    "id" : "688315e2-be63-45c3-b63e-0a92874629df",
    "name" : "sensor-trend-tracker",
    "user_save_timestamp" : "1970-01-01T01:00:00.000Z",
    "auto_save_timestamp" : "1970-01-01T01:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "sparkNotebook" : null,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : [
      "org.apache.spark %% spark-streaming-kafka-0-8 % 2.1.0"
    ],
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : {
      "spark.streaming.kafka.maxRatePerPartition" : "500",
      "jars" : ""
    },
    "customVars" : null
  },
  "cells" : [
    {
      "metadata" : {
        "id" : "40AAFF23B9654D2A9B2E0C59515BAF8A"
      },
      "cell_type" : "markdown",
      "source" : "# Sensor Trend Tracker"
    },
    {
      "metadata" : {
        "id" : "D5BA242BD89A48F3816A0D7F53206849"
      },
      "cell_type" : "markdown",
      "source" : "In this notebook we quickly explore some specific aspects of multi-level state management in Spark Streaming\n\nBy combining local and distributed state, we implement a simple sensor trend tracker that can help us identify and report anomalies."
    },
    {
      "metadata" : {
        "id" : "79CD566650664F04A8515F55A809A44D"
      },
      "cell_type" : "markdown",
      "source" : "## Our Streaming dataset will consist of sensor information, containing the sensorId, a timestamp, and a value.\nThis component is a participant in a streaming pipeline.\n\nIt expects to receive moving averages of sensor data in the form of (id, timestamp, value) "
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "1BA7507DA513491F8D3A1AED38F087CB"
      },
      "cell_type" : "code",
      "source" : [
        "import org.apache.spark.streaming.Seconds\n",
        "val topic = \"sensor-processed\"\n",
        "val kafkaBootstrapServer = \"10.2.2.191:1025\"\n",
        "val threshold = 4.0\n",
        "val interval = Seconds(10) // seconds"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "import org.apache.spark.streaming.Seconds\ntopic: String = sensor-processed\nkafkaBootstrapServer: String = 10.2.2.191:1025\nthreshold: Double = 3.0\ninterval: org.apache.spark.streaming.Duration = 10000 ms\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 2,
          "time" : "Took: 1.019s, at 2017-11-30 16:38"
        }
      ]
    },
    {
      "metadata" : {
        "id" : "904E7DFFBF534F14832519CB2DF56D34"
      },
      "cell_type" : "markdown",
      "source" : "# Create a Streaming Standard Deviation Model\nBased on https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance"
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "C266DC9355EF4976A88F507075DCBE14"
      },
      "cell_type" : "code",
      "source" : [
        "case class M2(n:Int, mean: Double, m2:Double) {\n",
        "  def variance: Option[Double] = {\n",
        "    if (n<2) None else Some(m2/(n-1))\n",
        "  }\n",
        "  def stdev: Option[Double] = variance.map(Math.sqrt)\n",
        "  }\n",
        "  object M2 extends Serializable {\n",
        "    val Zero = M2(0,0.0,0.0)\n",
        "  }"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "defined class M2\ndefined object M2\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 3,
          "time" : "Took: 0.900s, at 2017-11-30 16:38"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "B919847213E345A6834B0B35E0D9F9C0"
      },
      "cell_type" : "code",
      "source" : [
        "// this needs to be outside of the class b/c of Spark Notebook serialization\n",
        "var entries:Map[String, M2] = Map.empty"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "entries: Map[String,M2] = Map()\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 4,
          "time" : "Took: 0.919s, at 2017-11-30 16:38"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "6C2DDCBFD1A24AE5AC93E34BEFC53FF8"
      },
      "cell_type" : "code",
      "source" : [
        "import org.apache.spark.rdd.RDD\n",
        "import org.apache.spark.SparkContext\n",
        "import org.apache.spark.streaming.dstream.DStream\n",
        "  \n",
        "class M2Model() extends Serializable {\n",
        "  \n",
        "  def trainOn(dstream: DStream[(String, Double)]): Unit = {\n",
        "    dstream.foreachRDD{rdd => \n",
        "                       val newEntriesRDD = rdd.map{case (id, x) => \n",
        "                                                val current = entries.get(id)\n",
        "                                                val updated = current.map{case M2(n, mean, m2) => {\n",
        "                                                  val np = n + 1\n",
        "                                                  val delta = x - mean\n",
        "                                                  val meanp = mean + delta/np\n",
        "                                                  val mp2 = m2 + delta*(x - meanp)\n",
        "                                                  (id, M2(np, meanp, mp2))\n",
        "                                                  }\n",
        "                                                 }.getOrElse(id -> M2.Zero)\n",
        "                                                 updated\n",
        "                                               }\n",
        "                       val newEntries: Array[(String, M2)] = newEntriesRDD.collect\n",
        "                       entries = entries ++ newEntries\n",
        "                      }\n",
        "  }\n",
        "  def predictOnValues(dstream: DStream[(String, Double)]): DStream[(String, Double, Double, Double)] = {\n",
        "    for { \n",
        "      (id, value) <- dstream\n",
        "      m2 <- entries.get(id)\n",
        "      stdev <- m2.stdev\n",
        "    } yield (id, value, m2.mean, stdev)\n",
        "  }\n",
        "}"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "<console>:52: warning: `withFilter' method does not yet exist on org.apache.spark.streaming.dstream.DStream[(String, Double)], using `filter' method instead\n             (id, value) <- dstream\n                            ^\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.SparkContext\nimport org.apache.spark.streaming.dstream.DStream\ndefined class M2Model\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 8,
          "time" : "Took: 0.941s, at 2017-11-30 16:39"
        }
      ]
    },
    {
      "metadata" : {
        "id" : "BFF9E80BAB5C4D468154BCD6FB5C6501"
      },
      "cell_type" : "markdown",
      "source" : "## We create our Streaming Context"
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "83B2B58C552240BBA6FF518B1AD274EB"
      },
      "cell_type" : "code",
      "source" : [
        "import org.apache.spark.streaming.StreamingContext\n",
        "@transient val streamingContext = new StreamingContext(sparkContext, interval)"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "import org.apache.spark.streaming.StreamingContext\nstreamingContext: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@3d4fdcb4\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 9,
          "time" : "Took: 1.056s, at 2017-11-30 16:39"
        }
      ]
    },
    {
      "metadata" : {
        "id" : "4E6DBF82BB6F4B6584233CD460F67263"
      },
      "cell_type" : "markdown",
      "source" : "## Our stream source will be a a Direct Kafka Stream\n"
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "DF03F66BDDE0447B8202D39F2C0202E2"
      },
      "cell_type" : "code",
      "source" : [
        "import org.apache.kafka.clients.consumer.ConsumerRecord\n",
        "import kafka.serializer.StringDecoder\n",
        "import org.apache.spark.streaming.kafka._\n",
        "\n",
        "val kafkaParams = Map[String, String](\n",
        "  \"metadata.broker.list\" -> kafkaBootstrapServer,\n",
        "  \"group.id\" -> \"sensor-tracker-group\",\n",
        "  \"auto.offset.reset\" -> \"largest\",\n",
        "  \"enable.auto.commit\" -> (false: java.lang.Boolean).toString\n",
        ")\n",
        "\n",
        "val topics = Set(topic)\n",
        "@transient val stream = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](\n",
        "     streamingContext, kafkaParams, topics)\n",
        "\n",
        "// kafka_010 APIs don't work on the Spark Notebook\n",
        "\n",
        "// @transient val stream = KafkaUtils.createDirectStream[String, String](\n",
        "//   streamingContext,\n",
        "//   PreferConsistent,\n",
        "//   Subscribe[String, String](topics, kafkaParams)\n",
        "// )\n",
        "\n"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "import org.apache.kafka.clients.consumer.ConsumerRecord\nimport kafka.serializer.StringDecoder\nimport org.apache.spark.streaming.kafka._\nkafkaParams: scala.collection.immutable.Map[String,String] = Map(metadata.broker.list -> 10.2.2.191:1025, group.id -> sensor-tracker-group, auto.offset.reset -> largest, enable.auto.commit -> false)\ntopics: scala.collection.immutable.Set[String] = Set(sensor-processed)\nstream: org.apache.spark.streaming.dstream.InputDStream[(String, String)] = org.apache.spark.streaming.kafka.DirectKafkaInputDStream@69e8a66d\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 10,
          "time" : "Took: 2.358s, at 2017-11-30 16:40"
        }
      ]
    },
    {
      "metadata" : {
        "id" : "CCCB597031E7451FB59D18BA85C0E4A4"
      },
      "cell_type" : "markdown",
      "source" : "# Providing Schema information for our streaming data\nNow that we have a DStream of fresh data processed in a 2-second interval, we can start focusing on the gist of this example.\nFirst, we want to define and apply a schema to the data we are receiving.\nIn Scala, we can define a schema with a `case class`"
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "E7A917C393654969812E6E38223BBA52"
      },
      "cell_type" : "code",
      "source" : [
        "case class SensorData(id: String, timestamp: Long, temp: Double)"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "defined class SensorData\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 11,
          "time" : "Took: 0.661s, at 2017-11-30 16:43"
        }
      ]
    },
    {
      "metadata" : {
        "id" : "CDF72D66AE6641E8B6D1D41BEAA87484"
      },
      "cell_type" : "markdown",
      "source" : "# Create our Model\nWe will train an online standard deviation algorithm and use it to score the incoming data."
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "B8733A11F3EF42A595D176FF91F39D4A"
      },
      "cell_type" : "code",
      "source" : [
        "val model = new M2Model()"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "model: M2Model = M2Model@6c24d68d\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 12,
          "time" : "Took: 0.974s, at 2017-11-30 16:43"
        }
      ]
    },
    {
      "metadata" : {
        "id" : "C5F61611DF7D431FA4208A02C738E4AF"
      },
      "cell_type" : "markdown",
      "source" : "# Convert the incoming JSON to `SensorData`"
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "0141D8A763F14F3A89A8AB0BCA0ADB5B"
      },
      "cell_type" : "code",
      "source" : [
        "val spark = sparkSession\n",
        "import spark.implicits._\n",
        "@transient val sensorDataStream = stream.transform{rdd => \n",
        "                                        val jsonData = rdd.map{case (k,v)  => v}\n",
        "                                        val ds = sparkSession.createDataset(jsonData)\n",
        "                                        val jsonDF = spark.read.json(ds)\n",
        "                                        val sensorDataDS = jsonDF.as[SensorData]\n",
        "                                        sensorDataDS.rdd\n",
        "                                       }"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@50088e2d\nimport spark.implicits._\nsensorDataStream: org.apache.spark.streaming.dstream.DStream[SensorData] = org.apache.spark.streaming.dstream.TransformedDStream@c86ef01\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 13,
          "time" : "Took: 2.210s, at 2017-11-30 16:43"
        }
      ]
    },
    {
      "metadata" : {
        "id" : "E28C393614DC43AC98FB04E6E4C23E32"
      },
      "cell_type" : "markdown",
      "source" : "# Prepare the stream to train our model"
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "A7350DE28CF14F72862E393EFBFF0885"
      },
      "cell_type" : "code",
      "source" : [
        "@transient val inputData = sensorDataStream.transform {sensorDataRDD =>  sensorDataRDD.map{case SensorData(id,ts,value) => (id, value)}}                                                            "
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "inputData: org.apache.spark.streaming.dstream.DStream[(String, Double)] = org.apache.spark.streaming.dstream.TransformedDStream@58e6ae53\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 14,
          "time" : "Took: 0.786s, at 2017-11-30 16:43"
        }
      ]
    },
    {
      "metadata" : {
        "id" : "B3467959C76443D8835F5FD18CD5F232"
      },
      "cell_type" : "markdown",
      "source" : "## Use the data to train the model"
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "4439737C940E49FDACC1678A336BF926"
      },
      "cell_type" : "code",
      "source" : [
        "model.trainOn(inputData)"
      ],
      "outputs" : [
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 15,
          "time" : "Took: 0.907s, at 2017-11-30 16:43"
        }
      ]
    },
    {
      "metadata" : {
        "id" : "E9B2777558F2465D829248EBC0AEE0FB"
      },
      "cell_type" : "markdown",
      "source" : "## Score the streaming data using the trained model"
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "A36AA2E7377C4E15919C5A6C7B947B88"
      },
      "cell_type" : "code",
      "source" : [
        "@transient val scored = model.predictOnValues(inputData)"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "scored: org.apache.spark.streaming.dstream.DStream[(String, Double, Double, Double)] = org.apache.spark.streaming.dstream.FlatMappedDStream@62bc6a9e\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 16,
          "time" : "Took: 0.752s, at 2017-11-30 16:43"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "DF831BD248614BC89C650598F9B24F3A"
      },
      "cell_type" : "code",
      "source" : [
        "val scatterChart = new ScatterChart(Seq((0.0,0.0)))\n",
        "scatterChart\n"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "scatterChart: notebook.front.widgets.charts.ScatterChart[Seq[(Double, Double)]] = <ScatterChart widget>\nres20: notebook.front.widgets.charts.ScatterChart[Seq[(Double, Double)]] = <ScatterChart widget>\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : "<div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anon6289fcfa7a9584ab8937bdf4383de622&quot;,&quot;dataInit&quot;:[{&quot;_1&quot;:0,&quot;_2&quot;:0}],&quot;genId&quot;:&quot;425862736&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/scatterChart'], \n      function(playground, _magicscatterChart) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magicscatterChart,\n    \"o\": {\"x\":\"_1\",\"y\":\"_2\",\"width\":600,\"height\":400}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    <div>\n      <span class=\"chart-total-item-count\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anon66a619d5a97f89ff4198319e04e9ab40&quot;,&quot;initialValue&quot;:&quot;1&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p> entries total</span>\n      <span class=\"chart-sampling-warning\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anon5e4e57375b07c59429a5e9eecfa98198&quot;,&quot;initialValue&quot;:&quot;&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p></span>\n      <div>\n      </div>\n    </div></div>"
          },
          "output_type" : "execute_result",
          "execution_count" : 19,
          "time" : "Took: 1.107s, at 2017-11-30 16:46"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "489F976ACDEA435D874E144E9E9F00E2"
      },
      "cell_type" : "code",
      "source" : [
        "scored.foreachRDD{rdd =>\n",
        "  val data = rdd.collect.map{case (id, value, mean, std) => (value, std)}\n",
        "  scatterChart.applyOn(data)\n",
        "}"
      ],
      "outputs" : [
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 21,
          "time" : "Took: 1.082s, at 2017-11-30 16:48"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "EA5A8CC609964B7488A090B21360C4F1"
      },
      "cell_type" : "code",
      "source" : [
        "val outputBox = ul(20)\n",
        "outputBox.append(\".\")\n",
        "outputBox"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "outputBox: notebook.front.widgets.HtmlList = <HtmlList widget>\nres26: notebook.front.widgets.HtmlList = <HtmlList widget>\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : "<ul data-bind=\"foreach: value\"><li data-bind=\"html: $data\"></li><script data-this=\"{&quot;valueId&quot;:&quot;anon0ba9cf680b575f0006eae6b63eefa263&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId)\n    },\n    this\n  );\n});\n                            /*]]>*/</script></ul>"
          },
          "output_type" : "execute_result",
          "execution_count" : 22,
          "time" : "Took: 1.118s, at 2017-11-30 16:48"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "B7690B3120FD4A2193BC5800B37C4F9D"
      },
      "cell_type" : "code",
      "source" : [
        "val debugBox = ul(20)\n",
        "debugBox"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "debugBox: notebook.front.widgets.HtmlList = <HtmlList widget>\nres28: notebook.front.widgets.HtmlList = <HtmlList widget>\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : "<ul data-bind=\"foreach: value\"><li data-bind=\"html: $data\"></li><script data-this=\"{&quot;valueId&quot;:&quot;anonbcd2e18172a006edd76cc4bc920ce5c6&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId)\n    },\n    this\n  );\n});\n                            /*]]>*/</script></ul>"
          },
          "output_type" : "execute_result",
          "execution_count" : 23,
          "time" : "Took: 0.838s, at 2017-11-30 16:48"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "73F95E77619145E2BC3A4250D7734B96"
      },
      "cell_type" : "code",
      "source" : [
        "@transient val suspects = scored.filter{case (id, value, mean, std) => (value > mean + std * threshold) || (value < mean - std * threshold) }"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "suspects: org.apache.spark.streaming.dstream.DStream[(String, Double, Double, Double)] = org.apache.spark.streaming.dstream.FilteredDStream@250687d7\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 24,
          "time" : "Took: 0.780s, at 2017-11-30 16:49"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "9133EFDA4CAB4DAF8ACA0E6C753E2A9E"
      },
      "cell_type" : "code",
      "source" : [
        "suspects.foreachRDD{rdd => \n",
        "                    val sample = rdd.take(20).map(_.toString)\n",
        "                    val total = s\"total found: ${rdd.count}\"\n",
        "                    outputBox(total +: sample)\n",
        "                    \n",
        "                   }                  "
      ],
      "outputs" : [
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 25,
          "time" : "Took: 0.881s, at 2017-11-30 16:49"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "D691396F838E4087971FFDBFB3E06B7D"
      },
      "cell_type" : "code",
      "source" : [
        "inputData.foreachRDD{rdd => \n",
        "                    val sample = rdd.take(20).map(_.toString)\n",
        "                    debugBox.appendAll(sample)\n",
        "                   } "
      ],
      "outputs" : [
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 26,
          "time" : "Took: 1.082s, at 2017-11-30 16:49"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "F366201F2275412F818532AB671A55BC"
      },
      "cell_type" : "code",
      "source" : [
        "streamingContext.start()"
      ],
      "outputs" : [
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 27,
          "time" : "Took: 1.025s, at 2017-11-30 16:49"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "B6F0075E9BB04467858CABAA000489EF"
      },
      "cell_type" : "code",
      "source" : [
        "// Be careful not to stop the context if you want the streaming process to continue\n",
        "streamingContext.stop(false)"
      ],
      "outputs" : [
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 30,
          "time" : "Took: 1.534s, at 2017-11-30 16:57"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "presentation" : {
          "tabs_state" : "{\n  \"tab_id\": \"#tab1364664082-0\"\n}",
          "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
        },
        "id" : "AFBE5F4FE7384EC6A5E4448760031620"
      },
      "cell_type" : "code",
      "source" : [
        "entries(\"dth-001\").stdev"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "res37: Option[Double] = Some(0.1356008026337026)\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : "Some(0.1356008026337026)"
          },
          "output_type" : "execute_result",
          "execution_count" : 28,
          "time" : "Took: 0.650s, at 2017-11-30 16:53"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "EA2B7160FB97420BAFFDF259FE678EA9"
      },
      "cell_type" : "code",
      "source" : [
        "entries(\"dth-001\")"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "res39: M2 = M2(19,21.931393522267214,0.33132288867019494)\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : "M2(19,21.931393522267214,0.33132288867019494)"
          },
          "output_type" : "execute_result",
          "execution_count" : 29,
          "time" : "Took: 0.788s, at 2017-11-30 16:53"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "5458571C99D44C8B9DB0FC5E1C510863"
      },
      "cell_type" : "code",
      "source" : [
        ""
      ],
      "outputs" : [ ]
    }
  ],
  "nbformat" : 4
}