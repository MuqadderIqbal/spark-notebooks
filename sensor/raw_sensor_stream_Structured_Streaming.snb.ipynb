{
  "metadata" : {
    "id" : "0da40699-353a-45b3-b9a7-41e64b455005",
    "name" : "raw_sensor_stream_Structured_Streaming",
    "user_save_timestamp" : "1970-01-01T01:00:00.000Z",
    "auto_save_timestamp" : "1970-01-01T01:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "sparkNotebook" : null,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : [
      "org.apache.spark %% spark-sql-kafka-0-10 % 2.2.0"
    ],
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : null,
    "customVars" : null
  },
  "cells" : [
    {
      "metadata" : {
        "id" : "6E2995E02B244E978E1327B7A60484F0"
      },
      "cell_type" : "markdown",
      "source" : "# Reading Sensor Data from Kafka with Structured Streaming\n\nThe intention of this example is to explore the main aspects of the Structured Streaming API.\n\nWe will: \n - use the Kafka `source` to consume events from the `sensor-raw` topic in Kafka\n - implement the application logic using the Dataset API\n - use the `memory` sink to visualize the data\n - use the `kafka` sink to publish our results to a different topic and make it available downstream.\n - have some fun!  "
    },
    {
      "metadata" : {
        "id" : "CA367DA2510C4533840296E5D4704D43"
      },
      "cell_type" : "markdown",
      "source" : "##Common Definitions\nWe define a series of parameters common to  the notebook"
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "3B783C2DA5A9409E85DF2CE7F061AECB"
      },
      "cell_type" : "code",
      "source" : [
        "val sourceTopic = \"sensor-raw\"\n",
        "val targetTopic = \"sensor-processed\"\n",
        "//val kafkaBootstrapServer = \"172.17.0.2:9092\" // local\n",
        "val kafkaBootstrapServer = \"10.2.2.191:1025\" // fast-data-ec2"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "id" : "B42E8EC94CAC48A093E2E1ED7CF84E3F"
      },
      "cell_type" : "markdown",
      "source" : "## Read a stream from Kafka\nWe use the kafka source to subscribe to the `sourceTopic` that contains the raw sensor data.\nThis results in a streaming dataframe that we use to operate on the underlying data"
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "3225F1A3939642B087F1BE6A37EB9D03"
      },
      "cell_type" : "code",
      "source" : [
        "val rawData = sparkSession.readStream\n",
        "      .format(\"kafka\")\n",
        "      .option(\"kafka.bootstrap.servers\", kafkaBootstrapServer)\n",
        "      .option(\"subscribe\", sourceTopic)\n",
        "      .option(\"startingOffsets\", \"latest\")\n",
        "      .load()"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "497CEEFAB7DF40F884CC7A8139C3DA5F"
      },
      "cell_type" : "code",
      "source" : [
        "rawData.isStreaming"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "A74BF086DCC240168F21E57797088678"
      },
      "cell_type" : "code",
      "source" : [
        "rawData.printSchema()"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "id" : "F2782E9257024A819A1ADAAC2EFB1238"
      },
      "cell_type" : "markdown",
      "source" : "## Declare the schema of the data in the stream \nWe need to declare the schema of the data in the stream in order to parse it.\n\nWe use a case class to define the schema. It's much more convenient that using the sql types directly."
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "A39D7FB1A9AC496F8DFC7502EB0A4C29"
      },
      "cell_type" : "code",
      "source" : [
        "case class SensorData(id: String, ts: Long, temp: Double, hum: Double)"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "89E72AB850BC4AE7812D0236B3F51CED"
      },
      "cell_type" : "code",
      "source" : [
        "import org.apache.spark.sql.Encoders\n",
        "val schema = Encoders.product[SensorData].schema"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "id" : "1DB49CA4BD7E4BA19AFBECF7D195CF27"
      },
      "cell_type" : "markdown",
      "source" : "## Parse the Data\nThe actual payload is contained in the 'value' field that we get from the kafka topic (see above).\nWe first need to convert that binary value field to string and then use the `json` support in Spark to transform our incoming data into a structured streaming `Dataset`\n"
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "6E729EE3495E4AA88FC9E347BDEE3210"
      },
      "cell_type" : "code",
      "source" : [
        "val rawValues = rawData.selectExpr(\"CAST(value AS STRING)\").as[String]\n",
        "val jsonValues = rawValues.select(from_json($\"value\", schema) as \"record\")\n",
        "val sensorData = jsonValues.select(\"record.*\").as[SensorData]"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "F69903037F07494EA9A42815B386C0A4"
      },
      "cell_type" : "code",
      "source" : [
        "sensorData.printSchema()"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "id" : "E579B7773A82414885636B662D189409"
      },
      "cell_type" : "markdown",
      "source" : "## Explore the data stream\nTo view the streaming data, we will use the `memory` sink and query the resulting table to get samples of the data."
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "0A62B3731D884D1185F153937359EB80"
      },
      "cell_type" : "code",
      "source" : [
        "val visualizationQuery = sensorData.writeStream\n",
        "  .queryName(\"visualization\")    // this query name will be the SQL table name\n",
        "  .outputMode(\"append\")\n",
        "  .format(\"memory\")\n",
        "  .start()"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "id" : "07C88930FB5446E48A517CD074000D17"
      },
      "cell_type" : "markdown",
      "source" : "## Explore the Execution\nWe can use the queryProgressObject to examine the performance characteristics of the on-going query"
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "presentation" : {
          "tabs_state" : "{\n  \"tab_id\": \"#tab66994859-0\"\n}",
          "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
        },
        "id" : "C60D42C6AA7C43108AF52A4EEA7B407E"
      },
      "cell_type" : "code",
      "source" : [
        "val progress = visualizationQuery.recentProgress"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "presentation" : {
          "tabs_state" : "{\n  \"tab_id\": \"#tab985946322-1\"\n}",
          "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [\n    \"_1\"\n  ],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Line Chart\"\n}"
        },
        "id" : "7B8E1B50F1BC435A8FBE4FA84DBBBADB"
      },
      "cell_type" : "code",
      "source" : [
        "progress.map(entry  => (entry.inputRowsPerSecond, entry.processedRowsPerSecond))"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "id" : "2E96BBE34C994DEA8CD5200022660825"
      },
      "cell_type" : "markdown",
      "source" : "## Explore the Data\nThe `memory` sink creates an in-memory SQL table (like a `tempTable`) that we can query using Spark SQL\nThe result of the query is a static `Dataframe` that contains a snapshot of the data."
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "3A8498745D0E4A86BFB12ED10544529E"
      },
      "cell_type" : "code",
      "source" : [
        "val sampleDataset = sparkSession.sql(\"select * from visualization\")"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "1EED0786D37745B98B276F84B6661364"
      },
      "cell_type" : "code",
      "source" : [
        "// This is a static Dataset!\n",
        "sampleDataset.isStreaming"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "id" : "482CE35A1729444EBCAEC703D0D44EDA"
      },
      "cell_type" : "markdown",
      "source" : "### Our dataset is backed by the streaming data, it will update each time we execute an action, delivering the latest data."
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "A7BE1142BA7F478DBD40DD167000E930"
      },
      "cell_type" : "code",
      "source" : [
        "sampleDataset.count"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "id" : "0693F6BC6F6E422281CFDB057D95F707"
      },
      "cell_type" : "markdown",
      "source" : "## Visualize the Data\nWe will make a custom live update by querying the stream every so often for the latest updates"
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "6D330DDECB47432C8B2C534C9E9291FF"
      },
      "cell_type" : "code",
      "source" : [
        "val dummy = Seq((System.currentTimeMillis, 0.1))\n",
        "\n",
        "val chart = CustomPlotlyChart(dummy,\n",
        "                  layout=s\"{title: 'sensor data sample'}\",\n",
        "                  dataOptions=\"\"\"{type: 'line'}\"\"\",\n",
        "                  dataSources=\"{x: '_1', y: '_2' }\")\n",
        "chart"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "id" : "C96D62E1D7754A2F8C9CCCB23C2D4E29"
      },
      "cell_type" : "markdown",
      "source" : "## Async update of our visualization\nWe will use a plain old Thread to run a recurrent query on our in-memory table and update the chart accordingly.\n"
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "D496C339A4F24B9F8038D2DA970220E5"
      },
      "cell_type" : "code",
      "source" : [
        "@volatile var running = true"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "645C005ED2D843EF81E0A96329F9C7D4"
      },
      "cell_type" : "code",
      "source" : [
        "import scala.concurrent.duration._\n",
        "import scala.annotation.tailrec\n",
        "\n",
        "val updater = new Thread() {\n",
        "  @tailrec\n",
        "  def visualize(): Unit = {\n",
        "    val lastMinute = System.currentTimeMillis - 1.minute.toMillis\n",
        "    val data = sampleDataset.where($\"ts\" > lastMinute and $\"id\" === \"dth-001\").as[SensorData]\n",
        "                            .map{case SensorData(id, ts, temp, hum) => (ts/1000%3600, temp)}.collect\n",
        "    chart.applyOn(data)\n",
        "    if (running) {\n",
        "      Thread.sleep(1.second.toMillis)\n",
        "      visualize()\n",
        "    } else ()\n",
        "  } \n",
        "  \n",
        "  override def run() {\n",
        "    visualize()\n",
        "  }\n",
        "}.start()\n"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "5F80EB02EB934828854A917DBF7D0DE1"
      },
      "cell_type" : "code",
      "source" : [
        "visualizationQuery.stop()"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "5FBE35E1590E4CA88F05B9A4BF8CF794"
      },
      "cell_type" : "code",
      "source" : [
        "running = false"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "A8292D947D7E4A889B416F5C2BB158C7"
      },
      "cell_type" : "markdown",
      "source" : "# Improve the data with sliding windows"
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "BD00015ED0564F9B9F405737792E7743"
      },
      "cell_type" : "code",
      "source" : [
        "import org.apache.spark.sql.types._"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "BBDA2F693B6247DD8849298DA0BC769B"
      },
      "cell_type" : "code",
      "source" : [
        "val toSeconds = udf((ts:Long) => ts/1000)\n",
        "val tempBySensorMovingAverage = sensorData.withColumn(\"timestamp\", toSeconds($\"ts\").cast(TimestampType))\n",
        "                                          .withWatermark(\"timestamp\", \"30 seconds\")\n",
        "                                          .groupBy($\"id\", window($\"timestamp\", \"30 seconds\", \"10 seconds\"))\n",
        "                                          .agg(avg($\"temp\"))"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "9BF82601363C40579E8DBC9AF9AC2F89"
      },
      "cell_type" : "code",
      "source" : [
        "tempBySensorMovingAverage.printSchema"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "414735A902F34CDE84D1C3642D512973"
      },
      "cell_type" : "code",
      "source" : [
        "val windowedSensorQuery = tempBySensorMovingAverage.writeStream\n",
        "  .queryName(\"movingAverage4\")    // this query name will be the table name\n",
        "  .outputMode(\"append\")  \n",
        "  .format(\"memory\")\n",
        "  .start()"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "89EE4EF3EFA845D987484FE216C865A3"
      },
      "cell_type" : "code",
      "source" : [
        "val movingAvgDF = sparkSession.sql(\"select * from movingAverage4\")"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "2E71B78A71D543358203125FE62FE6F5"
      },
      "cell_type" : "code",
      "source" : [
        "movingAvgDF"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "AA5DF0B0AD6D4EF093A9879DB10A106E"
      },
      "cell_type" : "code",
      "source" : [
        "import org.apache.spark.sql.functions._\n",
        "val lastMinute: Long = System.currentTimeMillis/1000 - 5.minute.toSeconds\n",
        "val mAvgSample = movingAvgDF.select($\"window.start\".cast(LongType) as \"timestamp\", $\"avg(temp)\" as \"temp\")\n",
        "                   .where($\"timestamp\" > lastMinute and $\"id\" === \"dth-001\")\n",
        "                   .orderBy($\"timestamp\")\n",
        "                   .as[(Long, Double)]\n",
        "                   .collect().map{case (ts, v) => (ts  % 3600,v)}\n",
        "\n",
        "\n",
        "CustomPlotlyChart(mAvgSample,\n",
        "                  layout=s\"{title: 'moving average sensor data'}\",\n",
        "                  dataOptions=\"\"\"{type: 'line'}\"\"\",\n",
        "                  dataSources=\"{x: '_1', y: '_2'}\")"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "B39288CF67F04535870AE968E0700074"
      },
      "cell_type" : "code",
      "source" : [
        "// stop the ancilliary visualization query\n",
        "windowedSensorQuery.stop()"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "id" : "495EC81428014BC4AE7CD9DE863EFB06"
      },
      "cell_type" : "markdown",
      "source" : "## Write our moving average data to our `sensor-clean` topic"
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "707C86245D9542F384FFC006900AA88D"
      },
      "cell_type" : "code",
      "source" : [
        "val kafkaFormat = tempBySensorMovingAverage.select($\"id\", $\"window.start\".cast(LongType) as \"timestamp\", $\"avg(temp)\" as \"temp\")\n",
        "                                                  .select($\"id\" as \"key\", to_json(struct($\"id\", $\"timestamp\", $\"temp\")) as \"value\")"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "AF7660F21370444A89C197C94930052E"
      },
      "cell_type" : "code",
      "source" : [
        "val kafkaWriterQuery = kafkaFormat.writeStream\n",
        "  .queryName(\"kafkaWriter\")    // this query name will be the table name\n",
        "  .outputMode(\"append\") \n",
        "  .format(\"kafka\")\n",
        "  .option(\"kafka.bootstrap.servers\", kafkaBootstrapServer)\n",
        "  .option(\"topic\", targetTopic)\n",
        "  .option(\"checkpointLocation\", \"/tmp/spark/checkpoint2\")\n",
        "  .option(\"failOnDataLoss\", \"false\")\n",
        "  .start()"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "presentation" : {
          "tabs_state" : "{\n  \"tab_id\": \"#tab289254220-0\"\n}",
          "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
        },
        "id" : "F359A73CFAF04B978876322746ECF620"
      },
      "cell_type" : "code",
      "source" : [
        "kafkaWriterQuery.recentProgress"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "06AFEE5AE0764ECCB6F01239CF4267B9"
      },
      "cell_type" : "code",
      "source" : [
        ""
      ],
      "outputs" : [ ]
    }
  ],
  "nbformat" : 4
}