{
  "metadata" : {
    "id" : "0da40699-353a-45b3-b9a7-41e64b455005",
    "name" : "raw_sensor_stream_Structured_Streaming",
    "user_save_timestamp" : "1970-01-01T01:00:00.000Z",
    "auto_save_timestamp" : "1970-01-01T01:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "sparkNotebook" : null,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : [ "org.apache.spark %% spark-sql-kafka-0-10 % 2.2.0" ],
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : null,
    "customVars" : null
  },
  "cells" : [ {
    "metadata" : {
      "id" : "6E2995E02B244E978E1327B7A60484F0"
    },
    "cell_type" : "markdown",
    "source" : "# Reading Sensor Data from Kafka with Structured Streaming\n\nThe intention of this example is to explore the main aspects of the Structured Streaming API.\n\nWe will: \n - use the Kafka `source` to consume events from the `sensor-raw` topic in Kafka\n - implement the application logic using the Dataset API\n - use the `memory` sink to visualize the data\n - use the `kafka` sink to publish our results to a different topic and make it available downstream.\n - have some fun!  "
  }, {
    "metadata" : {
      "id" : "CA367DA2510C4533840296E5D4704D43"
    },
    "cell_type" : "markdown",
    "source" : "##Common Definitions\nWe define a series of parameters common to  the notebook"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "3B783C2DA5A9409E85DF2CE7F061AECB"
    },
    "cell_type" : "code",
    "source" : [ "val sourceTopic = \"sensor-raw\"\n", "val targetTopic = \"sensor-processed\"\n", "val kafkaBootstrapServer = \"172.17.0.2:9092\" // local\n", "//val kafkaBootstrapServer = \"10.2.2.191:1025\" // fast-data-ec2" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "sourceTopic: String = sensor-raw\ntargetTopic: String = sensor-processed\nkafkaBootstrapServer: String = 172.17.0.2:9092\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 1,
      "time" : "Took: 0.989s, at 2018-05-23 12:29"
    } ]
  }, {
    "metadata" : {
      "id" : "B42E8EC94CAC48A093E2E1ED7CF84E3F"
    },
    "cell_type" : "markdown",
    "source" : "## Read a stream from Kafka\nWe use the kafka source to subscribe to the `sourceTopic` that contains the raw sensor data.\nThis results in a streaming dataframe that we use to operate on the underlying data"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "3225F1A3939642B087F1BE6A37EB9D03"
    },
    "cell_type" : "code",
    "source" : [ "val rawData = sparkSession.readStream\n", "      .format(\"kafka\")\n", "      .option(\"kafka.bootstrap.servers\", kafkaBootstrapServer)\n", "      .option(\"subscribe\", sourceTopic)\n", "      .option(\"startingOffsets\", \"latest\")\n", "      .load()" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "rawData: org.apache.spark.sql.DataFrame = [key: binary, value: binary ... 5 more fields]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 3,
      "time" : "Took: 0.773s, at 2018-05-23 12:30"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "497CEEFAB7DF40F884CC7A8139C3DA5F"
    },
    "cell_type" : "code",
    "source" : [ "rawData.isStreaming" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res4: Boolean = true\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "true"
      },
      "output_type" : "execute_result",
      "execution_count" : 4,
      "time" : "Took: 0.873s, at 2018-05-23 12:30"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "A74BF086DCC240168F21E57797088678"
    },
    "cell_type" : "code",
    "source" : [ "rawData.printSchema()" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "root\n |-- key: binary (nullable = true)\n |-- value: binary (nullable = true)\n |-- topic: string (nullable = true)\n |-- partition: integer (nullable = true)\n |-- offset: long (nullable = true)\n |-- timestamp: timestamp (nullable = true)\n |-- timestampType: integer (nullable = true)\n\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 5,
      "time" : "Took: 0.838s, at 2018-05-23 12:30"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "F81993C46783437391F5803413D7DDD1"
    },
    "cell_type" : "code",
    "source" : [ "// There are Dataframe operations that don't make sense over a stream\n", "rawData.count()" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "org.apache.spark.sql.AnalysisException: Queries with streaming sources must be executed with writeStream.start();;\nkafka\n  at org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$.org$apache$spark$sql$catalyst$analysis$UnsupportedOperationChecker$$throwError(UnsupportedOperationChecker.scala:297)\n  at org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$$anonfun$checkForBatch$1.apply(UnsupportedOperationChecker.scala:36)\n  at org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$$anonfun$checkForBatch$1.apply(UnsupportedOperationChecker.scala:34)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n  at scala.collection.immutable.List.foreach(List.scala:381)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n  at org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$.checkForBatch(UnsupportedOperationChecker.scala:34)\n  at org.apache.spark.sql.execution.QueryExecution.assertSupported(QueryExecution.scala:63)\n  at org.apache.spark.sql.execution.QueryExecution.withCachedData$lzycompute(QueryExecution.scala:74)\n  at org.apache.spark.sql.execution.QueryExecution.withCachedData(QueryExecution.scala:72)\n  at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:78)\n  at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:78)\n  at org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:84)\n  at org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:80)\n  at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:89)\n  at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:89)\n  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:2837)\n  at org.apache.spark.sql.Dataset.count(Dataset.scala:2434)\n  ... 63 elided\n"
    } ]
  }, {
    "metadata" : {
      "id" : "E579B7773A82414885636B662D189409"
    },
    "cell_type" : "markdown",
    "source" : "## Explore the data stream\nTo view the streaming data, we will use the `memory` sink and query the resulting table to get samples of the data."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "0897F581692343B38924643850C046EC"
    },
    "cell_type" : "code",
    "source" : [ "val rawDataStream = rawData.writeStream\n", "  .queryName(\"raw_data\")    // this query name will be the SQL table name\n", "  .outputMode(\"append\")\n", "  .format(\"memory\")\n", "  .start()" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "rawDataStream: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@5ce5babf\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 7,
      "time" : "Took: 0.967s, at 2018-05-23 12:32"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "72274BA5FF5E4F888FE737C2B8E0F4D4"
    },
    "cell_type" : "code",
    "source" : [ "val sampleDataset = sparkSession.sql(\"select * from raw_data\")" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "sampleDataset: org.apache.spark.sql.DataFrame = [key: binary, value: binary ... 5 more fields]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 8,
      "time" : "Took: 0.746s, at 2018-05-23 12:32"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "CEBD77C6E46342A3B42E1013025B1586"
    },
    "cell_type" : "code",
    "source" : [ "sampleDataset.isStreaming" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res12: Boolean = false\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "false"
      },
      "output_type" : "execute_result",
      "execution_count" : 9,
      "time" : "Took: 0.867s, at 2018-05-23 12:32"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "1901017ECDD74096BB772F4AB7AE59F5"
    },
    "cell_type" : "code",
    "source" : [ "sampleDataset.show()" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "+----+--------------------+----------+---------+------+--------------------+-------------+\n| key|               value|     topic|partition|offset|           timestamp|timestampType|\n+----+--------------------+----------+---------+------+--------------------+-------------+\n|null|[7B 22 69 64 22 3...|sensor-raw|        0| 25229|1970-01-01 00:59:...|            0|\n|null|[7B 22 69 64 22 3...|sensor-raw|        0| 25230|1970-01-01 00:59:...|            0|\n|null|[7B 22 69 64 22 3...|sensor-raw|        0| 25231|1970-01-01 00:59:...|            0|\n|null|[7B 22 69 64 22 3...|sensor-raw|        0| 25232|1970-01-01 00:59:...|            0|\n|null|[7B 22 69 64 22 3...|sensor-raw|        0| 25233|1970-01-01 00:59:...|            0|\n|null|[7B 22 69 64 22 3...|sensor-raw|        0| 25234|1970-01-01 00:59:...|            0|\n|null|[7B 22 69 64 22 3...|sensor-raw|        0| 25235|1970-01-01 00:59:...|            0|\n|null|[7B 22 69 64 22 3...|sensor-raw|        0| 25236|1970-01-01 00:59:...|            0|\n|null|[7B 22 69 64 22 3...|sensor-raw|        0| 25237|1970-01-01 00:59:...|            0|\n|null|[7B 22 69 64 22 3...|sensor-raw|        0| 25238|1970-01-01 00:59:...|            0|\n|null|[7B 22 69 64 22 3...|sensor-raw|        0| 25239|1970-01-01 00:59:...|            0|\n|null|[7B 22 69 64 22 3...|sensor-raw|        0| 25240|1970-01-01 00:59:...|            0|\n|null|[7B 22 69 64 22 3...|sensor-raw|        0| 25241|1970-01-01 00:59:...|            0|\n|null|[7B 22 69 64 22 3...|sensor-raw|        0| 25242|1970-01-01 00:59:...|            0|\n|null|[7B 22 69 64 22 3...|sensor-raw|        0| 25243|1970-01-01 00:59:...|            0|\n|null|[7B 22 69 64 22 3...|sensor-raw|        0| 25244|1970-01-01 00:59:...|            0|\n|null|[7B 22 69 64 22 3...|sensor-raw|        0| 25245|1970-01-01 00:59:...|            0|\n|null|[7B 22 69 64 22 3...|sensor-raw|        0| 25246|1970-01-01 00:59:...|            0|\n|null|[7B 22 69 64 22 3...|sensor-raw|        0| 25247|1970-01-01 00:59:...|            0|\n|null|[7B 22 69 64 22 3...|sensor-raw|        0| 25248|1970-01-01 00:59:...|            0|\n+----+--------------------+----------+---------+------+--------------------+-------------+\nonly showing top 20 rows\n\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 10,
      "time" : "Took: 0.988s, at 2018-05-23 12:32"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "257AF9C1B7DB4C5885A53D1921CBBF7C"
    },
    "cell_type" : "code",
    "source" : [ "rawDataStream.stop()" ],
    "outputs" : [ {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 12,
      "time" : "Took: 0.698s, at 2018-05-23 12:33"
    } ]
  }, {
    "metadata" : {
      "id" : "1DB49CA4BD7E4BA19AFBECF7D195CF27"
    },
    "cell_type" : "markdown",
    "source" : "# Parse the Data\nThe actual payload is contained in the 'value' field that we get from the kafka topic (see above).\nWe first need to convert that binary value field to string and then use the `json` support in Spark to transform our incoming data into a structured streaming `Dataset`\n"
  }, {
    "metadata" : {
      "id" : "F2782E9257024A819A1ADAAC2EFB1238"
    },
    "cell_type" : "markdown",
    "source" : "## Declare the schema of the data in the stream \nWe need to declare the schema of the data in the stream in order to parse it.\n\nWe use a case class to define the schema. It's much more convenient that using the sql types directly."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "A39D7FB1A9AC496F8DFC7502EB0A4C29"
    },
    "cell_type" : "code",
    "source" : [ "case class SensorData(id: String, ts: Long, temp: Double, hum: Double)" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "defined class SensorData\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 13,
      "time" : "Took: 0.541s, at 2018-05-23 12:33"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "89E72AB850BC4AE7812D0236B3F51CED"
    },
    "cell_type" : "code",
    "source" : [ "import org.apache.spark.sql.Encoders\n", "val schema = Encoders.product[SensorData].schema" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import org.apache.spark.sql.Encoders\nschema: org.apache.spark.sql.types.StructType = StructType(StructField(id,StringType,true), StructField(ts,LongType,false), StructField(temp,DoubleType,false), StructField(hum,DoubleType,false))\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 14,
      "time" : "Took: 1.055s, at 2018-05-23 12:34"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "6E729EE3495E4AA88FC9E347BDEE3210"
    },
    "cell_type" : "code",
    "source" : [ "val rawValues = rawData.selectExpr(\"CAST(value AS STRING)\").as[String]\n", "val jsonValues = rawValues.select(from_json($\"value\", schema) as \"record\")\n", "val sensorData = jsonValues.select(\"record.*\").as[SensorData]" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "rawValues: org.apache.spark.sql.Dataset[String] = [value: string]\njsonValues: org.apache.spark.sql.DataFrame = [record: struct<id: string, ts: bigint ... 2 more fields>]\nsensorData: org.apache.spark.sql.Dataset[SensorData] = [id: string, ts: bigint ... 2 more fields]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 15,
      "time" : "Took: 0.842s, at 2018-05-23 12:34"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "F69903037F07494EA9A42815B386C0A4"
    },
    "cell_type" : "code",
    "source" : [ "sensorData.printSchema()" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "root\n |-- id: string (nullable = true)\n |-- ts: long (nullable = true)\n |-- temp: double (nullable = true)\n |-- hum: double (nullable = true)\n\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 16,
      "time" : "Took: 0.699s, at 2018-05-23 12:34"
    } ]
  }, {
    "metadata" : {
      "id" : "2E96BBE34C994DEA8CD5200022660825"
    },
    "cell_type" : "markdown",
    "source" : "## Explore the Data\nThe `memory` sink creates an in-memory SQL table (like a `tempTable`) that we can query using Spark SQL\nThe result of the query is a static `Dataframe` that contains a snapshot of the data."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "0A62B3731D884D1185F153937359EB80"
    },
    "cell_type" : "code",
    "source" : [ "val visualizationQuery = sensorData.writeStream\n", "  .queryName(\"visualization\")    // this query name will be the SQL table name\n", "  .outputMode(\"append\")\n", "  .format(\"memory\")\n", "  .start()" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "visualizationQuery: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@20f95bc4\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 17,
      "time" : "Took: 0.582s, at 2018-05-23 12:35"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "3A8498745D0E4A86BFB12ED10544529E"
    },
    "cell_type" : "code",
    "source" : [ "val sampleDataset = sparkSession.sql(\"select * from visualization\")" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "sampleDataset: org.apache.spark.sql.DataFrame = [id: string, ts: bigint ... 2 more fields]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 18,
      "time" : "Took: 0.699s, at 2018-05-23 12:35"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "1EED0786D37745B98B276F84B6661364"
    },
    "cell_type" : "code",
    "source" : [ "// This is a static Dataset!\n", "sampleDataset.isStreaming" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res27: Boolean = false\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "false"
      },
      "output_type" : "execute_result",
      "execution_count" : 19,
      "time" : "Took: 0.839s, at 2018-05-23 12:35"
    } ]
  }, {
    "metadata" : {
      "id" : "482CE35A1729444EBCAEC703D0D44EDA"
    },
    "cell_type" : "markdown",
    "source" : "### Our dataset is backed by the streaming data, it will update each time we execute an action, delivering the latest data."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "A7BE1142BA7F478DBD40DD167000E930"
    },
    "cell_type" : "code",
    "source" : [ "sampleDataset.count" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res29: Long = 232\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "232"
      },
      "output_type" : "execute_result",
      "execution_count" : 20,
      "time" : "Took: 1.218s, at 2018-05-23 12:35"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "6746B72D7F5D40EEA68EBBA2911BEF37"
    },
    "cell_type" : "code",
    "source" : [ "sampleDataset.count" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res33: Long = 613\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "613"
      },
      "output_type" : "execute_result",
      "execution_count" : 22,
      "time" : "Took: 0.796s, at 2018-05-23 12:35"
    } ]
  }, {
    "metadata" : {
      "id" : "0693F6BC6F6E422281CFDB057D95F707"
    },
    "cell_type" : "markdown",
    "source" : "## Visualize the Data\nWe will make a custom live update by querying the stream every so often for the latest updates"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "6D330DDECB47432C8B2C534C9E9291FF"
    },
    "cell_type" : "code",
    "source" : [ "val zero = Seq((System.currentTimeMillis, 0.1))\n", "\n", "val chart = CustomPlotlyChart(zero,\n", "                  layout=s\"{title: 'sensor data sample'}\",\n", "                  dataOptions=\"\"\"{type: 'line'}\"\"\",\n", "                  dataSources=\"{x: '_1', y: '_2' }\")\n", "chart" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "zero: Seq[(Long, Double)] = List((1527071737184,0.1))\nchart: notebook.front.widgets.charts.CustomPlotlyChart[Seq[(Long, Double)]] = <CustomPlotlyChart widget>\nres35: notebook.front.widgets.charts.CustomPlotlyChart[Seq[(Long, Double)]] = <CustomPlotlyChart widget>\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "<div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anonc521d6a2d88405b5132aa3b9a842fe4f&quot;,&quot;dataInit&quot;:[{&quot;_1&quot;:1527071737184,&quot;_2&quot;:0.1}],&quot;genId&quot;:&quot;1136906840&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/customPlotlyChart'], \n      function(playground, _magiccustomPlotlyChart) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magiccustomPlotlyChart,\n    \"o\": {\"js\":\"var layout = {title: 'sensor data sample'}; var dataSources={x: '_1', y: '_2' }; var dataOptions = {type: 'line'}; var extraOptions = {}\",\"headers\":[\"_1\",\"_2\"],\"height\":400}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    <div>\n      <span class=\"chart-total-item-count\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anona2fd0ae1aae15158cd4f6664956d0494&quot;,&quot;initialValue&quot;:&quot;1&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p> entries total</span>\n      <span class=\"chart-sampling-warning\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anon4679196265b245441375a9bb96f8dd3b&quot;,&quot;initialValue&quot;:&quot;&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p></span>\n      <div>\n      </div>\n    </div></div>"
      },
      "output_type" : "execute_result",
      "execution_count" : 23,
      "time" : "Took: 1.315s, at 2018-05-23 12:35"
    } ]
  }, {
    "metadata" : {
      "id" : "C96D62E1D7754A2F8C9CCCB23C2D4E29"
    },
    "cell_type" : "markdown",
    "source" : "## Async update of our visualization\nWe will use a plain old Thread to run a recurrent query on our in-memory table and update the chart accordingly.\n"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "D496C339A4F24B9F8038D2DA970220E5"
    },
    "cell_type" : "code",
    "source" : [ "@volatile var running = true" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "running: Boolean = true\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 24,
      "time" : "Took: 0.685s, at 2018-05-23 12:35"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "645C005ED2D843EF81E0A96329F9C7D4"
    },
    "cell_type" : "code",
    "source" : [ "import scala.concurrent.duration._\n", "import scala.annotation.tailrec\n", "\n", "val updater = new Thread() {\n", "  @tailrec\n", "  def visualize(): Unit = {\n", "    val lastMinute = System.currentTimeMillis - 1.minute.toMillis\n", "    val data = sampleDataset.where($\"ts\" > lastMinute and $\"id\" === \"office\").as[SensorData]\n", "                            .map{case SensorData(id, ts, temp, hum) => (ts/1000%3600, temp)}.collect\n", "    chart.applyOn(data)\n", "    if (running) {\n", "      Thread.sleep(1.second.toMillis)\n", "      visualize()\n", "    } else ()\n", "  } \n", "  \n", "  override def run() {\n", "    visualize()\n", "  }\n", "}.start()\n" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import scala.concurrent.duration._\nimport scala.annotation.tailrec\nupdater: Unit = ()\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 25,
      "time" : "Took: 1.213s, at 2018-05-23 12:35"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "5F80EB02EB934828854A917DBF7D0DE1"
    },
    "cell_type" : "code",
    "source" : [ "visualizationQuery.stop()" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "5FBE35E1590E4CA88F05B9A4BF8CF794"
    },
    "cell_type" : "code",
    "source" : [ "running = false" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "A8292D947D7E4A889B416F5C2BB158C7"
    },
    "cell_type" : "markdown",
    "source" : "# Using Event Time\n## Improving the data with sliding windows"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "BD00015ED0564F9B9F405737792E7743"
    },
    "cell_type" : "code",
    "source" : [ "import org.apache.spark.sql.types._" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import org.apache.spark.sql.types._\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 27,
      "time" : "Took: 0.544s, at 2018-05-23 12:39"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "BBDA2F693B6247DD8849298DA0BC769B"
    },
    "cell_type" : "code",
    "source" : [ "val toSeconds = udf((ts:Long) => ts/1000)\n", "val tempBySensorMovingAverage = sensorData.withColumn(\"timestamp\", toSeconds($\"ts\").cast(TimestampType))\n", "                                          .withWatermark(\"timestamp\", \"30 seconds\")\n", "                                          .groupBy($\"id\", window($\"timestamp\", \"30 seconds\", \"10 seconds\"))\n", "                                          .agg(avg($\"temp\"))" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "toSeconds: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,LongType,Some(List(LongType)))\ntempBySensorMovingAverage: org.apache.spark.sql.DataFrame = [id: string, window: struct<start: timestamp, end: timestamp> ... 1 more field]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 28,
      "time" : "Took: 1.029s, at 2018-05-23 12:39"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "9BF82601363C40579E8DBC9AF9AC2F89"
    },
    "cell_type" : "code",
    "source" : [ "tempBySensorMovingAverage.printSchema" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "root\n |-- id: string (nullable = true)\n |-- window: struct (nullable = true)\n |    |-- start: timestamp (nullable = true)\n |    |-- end: timestamp (nullable = true)\n |-- avg(temp): double (nullable = true)\n\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 30,
      "time" : "Took: 0.851s, at 2018-05-23 12:39"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "414735A902F34CDE84D1C3642D512973"
    },
    "cell_type" : "code",
    "source" : [ "val windowedSensorQuery = tempBySensorMovingAverage.writeStream\n", "  .queryName(\"moving_average\")    // this query name will be the table name\n", "  .outputMode(\"append\")  \n", "  .format(\"memory\")\n", "  .start()" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "windowedSensorQuery: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@7c4d5e31\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 31,
      "time" : "Took: 0.650s, at 2018-05-23 12:39"
    } ]
  }, {
    "metadata" : {
      "id" : "FF78BA44C8D34CCF8FDAF6E6E88CD0D8"
    },
    "cell_type" : "markdown",
    "source" : "### ------"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "89EE4EF3EFA845D987484FE216C865A3"
    },
    "cell_type" : "code",
    "source" : [ "val movingAvgDF = sparkSession.sql(\"select * from moving_average\")" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "movingAvgDF: org.apache.spark.sql.DataFrame = [id: string, window: struct<start: timestamp, end: timestamp> ... 1 more field]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 32,
      "time" : "Took: 0.826s, at 2018-05-23 12:43"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "2E71B78A71D543358203125FE62FE6F5"
    },
    "cell_type" : "code",
    "source" : [ "movingAvgDF" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res48: org.apache.spark.sql.DataFrame = [id: string, window: struct<start: timestamp, end: timestamp> ... 1 more field]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "<div class=\"df-canvas\">\n      <script data-this=\"{&quot;dataId&quot;:&quot;anoncbbfc797cb0b9de68d895092c95e0a60&quot;,&quot;partitionIndexId&quot;:&quot;anon6d13f1696fb5c4dc99d84b2bd8053690&quot;,&quot;numPartitions&quot;:546,&quot;dfSchema&quot;:{&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[{&quot;name&quot;:&quot;id&quot;,&quot;type&quot;:&quot;string&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;window&quot;,&quot;type&quot;:{&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[{&quot;name&quot;:&quot;start&quot;,&quot;type&quot;:&quot;timestamp&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;end&quot;,&quot;type&quot;:&quot;timestamp&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}}]},&quot;nullable&quot;:true,&quot;metadata&quot;:{&quot;spark.watermarkDelayMs&quot;:30000}},{&quot;name&quot;:&quot;avg(temp)&quot;,&quot;type&quot;:&quot;double&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}}]}}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/dataframe','../javascripts/notebook/consoleDir'], \n      function(dataframe, extension) {\n        dataframe.call(data, this, extension);\n      }\n    );/*]]>*/</script>\n      <link rel=\"stylesheet\" href=\"/assets/stylesheets/ipython/css/dataframe.css\" type=\"text/css\"/>\n    </div>"
      },
      "output_type" : "execute_result",
      "execution_count" : 33,
      "time" : "Took: 1.739s, at 2018-05-23 12:43"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "AA5DF0B0AD6D4EF093A9879DB10A106E"
    },
    "cell_type" : "code",
    "source" : [ "import org.apache.spark.sql.functions._\n", "val lastMinute: Long = System.currentTimeMillis/1000 - 5.minute.toSeconds\n", "val mAvgSample = movingAvgDF.select($\"window.start\".cast(LongType) as \"timestamp\", $\"avg(temp)\" as \"temp\")\n", "                   .where($\"timestamp\" > lastMinute and $\"id\" === \"office\")\n", "                   .orderBy($\"timestamp\")\n", "                   .as[(Long, Double)]\n", "                   .collect().map{case (ts, v) => (ts  % 3600,v)}\n", "\n", "\n", "CustomPlotlyChart(mAvgSample,\n", "                  layout=s\"{title: 'moving average sensor data'}\",\n", "                  dataOptions=\"\"\"{type: 'line'}\"\"\",\n", "                  dataSources=\"{x: '_1', y: '_2'}\")" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import org.apache.spark.sql.functions._\nlastMinute: Long = 1527071888\nmAvgSample: Array[(Long, Double)] = Array((2370,8.629999999999999), (2380,8.81060606060606), (2390,9.027169811320753), (2400,9.251833333333332), (2410,9.523500000000002), (2420,9.758166666666664), (2430,10.04466666666667), (2440,9.993166666666667), (2450,9.71133333333333), (2460,9.426333333333336), (2470,9.329833333333335), (2480,9.61683333333333), (2490,9.872333333333332), (2500,10.294666666666666), (2510,10.378000000000002), (2520,10.229333333333331))\nres52: notebook.front.widgets.charts.CustomPlotlyChart[Array[(Long, Double)]] = <CustomPlotlyChart widget>\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "<div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anon0f19a206760b0b190a3afad9ab09615e&quot;,&quot;dataInit&quot;:[{&quot;_1&quot;:2370,&quot;_2&quot;:8.629999999999999},{&quot;_1&quot;:2380,&quot;_2&quot;:8.81060606060606},{&quot;_1&quot;:2390,&quot;_2&quot;:9.027169811320753},{&quot;_1&quot;:2400,&quot;_2&quot;:9.251833333333332},{&quot;_1&quot;:2410,&quot;_2&quot;:9.523500000000002},{&quot;_1&quot;:2420,&quot;_2&quot;:9.758166666666664},{&quot;_1&quot;:2430,&quot;_2&quot;:10.04466666666667},{&quot;_1&quot;:2440,&quot;_2&quot;:9.993166666666667},{&quot;_1&quot;:2450,&quot;_2&quot;:9.71133333333333},{&quot;_1&quot;:2460,&quot;_2&quot;:9.426333333333336},{&quot;_1&quot;:2470,&quot;_2&quot;:9.329833333333335},{&quot;_1&quot;:2480,&quot;_2&quot;:9.61683333333333},{&quot;_1&quot;:2490,&quot;_2&quot;:9.872333333333332},{&quot;_1&quot;:2500,&quot;_2&quot;:10.294666666666666},{&quot;_1&quot;:2510,&quot;_2&quot;:10.378000000000002},{&quot;_1&quot;:2520,&quot;_2&quot;:10.229333333333331}],&quot;genId&quot;:&quot;1056012816&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/customPlotlyChart'], \n      function(playground, _magiccustomPlotlyChart) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magiccustomPlotlyChart,\n    \"o\": {\"js\":\"var layout = {title: 'moving average sensor data'}; var dataSources={x: '_1', y: '_2'}; var dataOptions = {type: 'line'}; var extraOptions = {}\",\"headers\":[\"_1\",\"_2\"],\"height\":400}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    <div>\n      <span class=\"chart-total-item-count\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anone042dae34642f82ede375b8a1bb75273&quot;,&quot;initialValue&quot;:&quot;16&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p> entries total</span>\n      <span class=\"chart-sampling-warning\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anonadaf526302704dc1ed89cf62a54d1231&quot;,&quot;initialValue&quot;:&quot;&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p></span>\n      <div>\n      </div>\n    </div></div>"
      },
      "output_type" : "execute_result",
      "execution_count" : 35,
      "time" : "Took: 1.541s, at 2018-05-23 12:43"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "B39288CF67F04535870AE968E0700074"
    },
    "cell_type" : "code",
    "source" : [ "// stop the ancilliary visualization query\n", "windowedSensorQuery.stop()" ],
    "outputs" : [ {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 36,
      "time" : "Took: 2.408s, at 2018-05-23 12:43"
    } ]
  }, {
    "metadata" : {
      "id" : "495EC81428014BC4AE7CD9DE863EFB06"
    },
    "cell_type" : "markdown",
    "source" : "## Write our moving average data to our `sensor-clean` topic"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "707C86245D9542F384FFC006900AA88D"
    },
    "cell_type" : "code",
    "source" : [ "val kafkaFormat = tempBySensorMovingAverage.select(\n", "  $\"id\", $\"window.start\".cast(LongType) as \"timestamp\", $\"avg(temp)\" as \"temp\")\n", "                                                  .select($\"id\" as \"key\", to_json(struct($\"id\", $\"timestamp\", $\"temp\")) as \"value\")" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "kafkaFormat: org.apache.spark.sql.DataFrame = [key: string, value: string]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 37,
      "time" : "Took: 0.683s, at 2018-05-23 12:43"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "AF7660F21370444A89C197C94930052E"
    },
    "cell_type" : "code",
    "source" : [ "val kafkaWriterQuery = kafkaFormat.writeStream\n", "  .queryName(\"kafkaWriter\")    // this query name will be the table name\n", "  .outputMode(\"append\") \n", "  .format(\"kafka\")\n", "  .option(\"kafka.bootstrap.servers\", kafkaBootstrapServer)\n", "  .option(\"topic\", targetTopic)\n", "  .option(\"checkpointLocation\", \"/tmp/spark/checkpoint2\")\n", "  .option(\"failOnDataLoss\", \"false\")\n", "  .start()" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "kafkaWriterQuery: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@4c91d31\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 38,
      "time" : "Took: 0.637s, at 2018-05-23 12:44"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab885999214-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "F359A73CFAF04B978876322746ECF620"
    },
    "cell_type" : "code",
    "source" : [ "kafkaWriterQuery.recentProgress" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res64: Array[org.apache.spark.sql.streaming.StreamingQueryProgress] = Array()\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "empty array"
      },
      "output_type" : "execute_result",
      "execution_count" : 42,
      "time" : "Took: 0.806s, at 2018-05-23 12:48"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "06AFEE5AE0764ECCB6F01239CF4267B9"
    },
    "cell_type" : "code",
    "source" : [ "" ],
    "outputs" : [ ]
  } ],
  "nbformat" : 4
}