{
  "metadata" : {
    "id" : "0da40699-353a-45b3-b9a7-41e64b455005",
    "name" : "raw_sensor_stream_Structured_Streaming",
    "user_save_timestamp" : "1970-01-01T01:00:00.000Z",
    "auto_save_timestamp" : "1970-01-01T01:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "sparkNotebook" : null,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : [ "org.apache.spark %% spark-sql-kafka-0-10 % 2.3.0" ],
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : null,
    "customVars" : null
  },
  "cells" : [ {
    "metadata" : {
      "id" : "6E2995E02B244E978E1327B7A60484F0"
    },
    "cell_type" : "markdown",
    "source" : "# Processing Sensor Data from Kafka with Structured Streaming\n\nThe intention of this example is to explore the main aspects of the Structured Streaming API.\n\nWe will: \n - use the Kafka `source` to consume events from the `sensor-raw` topic in Kafka\n - implement the application logic using the Dataset API\n - use the `memory` sink to visualize the data\n - use the `kafka` sink to publish our results to a different topic and make it available downstream.\n - have some fun!  "
  }, {
    "metadata" : {
      "id" : "CA367DA2510C4533840296E5D4704D43"
    },
    "cell_type" : "markdown",
    "source" : "##Common Definitions\nWe define a series of parameters common to  the notebook"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "3B783C2DA5A9409E85DF2CE7F061AECB"
    },
    "cell_type" : "code",
    "source" : [ "val sourceTopic = \"sensor-raw\"\n", "val targetTopic = \"sensor-processed\"\n", "val kafkaBootstrapServer = \"172.17.0.2:9092\" // local\n", "// val kafkaBootstrapServer = \"10.2.2.191:1025\" // fast-data-ec2" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "sourceTopic: String = sensor-raw\ntargetTopic: String = sensor-processed\nkafkaBootstrapServer: String = 172.17.0.2:9092\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 1,
      "time" : "Took: 1.274s, at 2018-11-14 10:04"
    } ]
  }, {
    "metadata" : {
      "id" : "B42E8EC94CAC48A093E2E1ED7CF84E3F"
    },
    "cell_type" : "markdown",
    "source" : "## Read a stream from Kafka\nWe use the kafka source to subscribe to the `sourceTopic` that contains the raw sensor data.\nThis results in a streaming dataframe that we use to operate on the underlying data"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "3225F1A3939642B087F1BE6A37EB9D03"
    },
    "cell_type" : "code",
    "source" : [ "val rawData = sparkSession.readStream\n", "      .format(\"kafka\")\n", "      .option(\"kafka.bootstrap.servers\", kafkaBootstrapServer)\n", "      .option(\"subscribe\", sourceTopic)\n", "      .option(\"startingOffsets\", \"latest\")\n", "      .load()" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "rawData: org.apache.spark.sql.DataFrame = [key: binary, value: binary ... 5 more fields]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 2,
      "time" : "Took: 1.885s, at 2018-11-14 10:04"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "497CEEFAB7DF40F884CC7A8139C3DA5F"
    },
    "cell_type" : "code",
    "source" : [ "rawData.isStreaming" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res3: Boolean = true\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "true"
      },
      "output_type" : "execute_result",
      "execution_count" : 3,
      "time" : "Took: 1.182s, at 2018-11-14 10:05"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "A74BF086DCC240168F21E57797088678"
    },
    "cell_type" : "code",
    "source" : [ "rawData.printSchema()" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "root\n |-- key: binary (nullable = true)\n |-- value: binary (nullable = true)\n |-- topic: string (nullable = true)\n |-- partition: integer (nullable = true)\n |-- offset: long (nullable = true)\n |-- timestamp: timestamp (nullable = true)\n |-- timestampType: integer (nullable = true)\n\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 4,
      "time" : "Took: 0.990s, at 2018-11-14 10:05"
    } ]
  }, {
    "metadata" : {
      "id" : "F2782E9257024A819A1ADAAC2EFB1238"
    },
    "cell_type" : "markdown",
    "source" : "## Declare the schema of the data in the stream\nWe need to declare the schema of the data in the stream in order to parse it.\n\nWe use a case class to define the schema. It's much more convenient that using the sql types directly."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "A39D7FB1A9AC496F8DFC7502EB0A4C29"
    },
    "cell_type" : "code",
    "source" : [ "case class SensorData(id: String, ts: Long, temp: Double, hum: Double)" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "defined class SensorData\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 5,
      "time" : "Took: 0.756s, at 2018-11-14 10:05"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "89E72AB850BC4AE7812D0236B3F51CED"
    },
    "cell_type" : "code",
    "source" : [ "import org.apache.spark.sql.Encoders\n", "val schema = Encoders.product[SensorData].schema" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import org.apache.spark.sql.Encoders\nschema: org.apache.spark.sql.types.StructType = StructType(StructField(id,StringType,true), StructField(ts,LongType,false), StructField(temp,DoubleType,false), StructField(hum,DoubleType,false))\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 6,
      "time" : "Took: 1.300s, at 2018-11-14 10:05"
    } ]
  }, {
    "metadata" : {
      "id" : "1DB49CA4BD7E4BA19AFBECF7D195CF27"
    },
    "cell_type" : "markdown",
    "source" : "## Parse the Data\nThe actual payload is contained in the 'value' field that we get from the kafka topic (see above).\nWe first need to convert that binary value field to string and then use the `json` support in Spark to transform our incoming data into a structured streaming `Dataset`\n"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "6E729EE3495E4AA88FC9E347BDEE3210"
    },
    "cell_type" : "code",
    "source" : [ "val rawValues = rawData.selectExpr(\"CAST(value AS STRING)\").as[String]\n", "val jsonValues = rawValues.select(from_json($\"value\", schema) as \"record\")\n", "val sensorData = jsonValues.select(\"record.*\").as[SensorData]" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "rawValues: org.apache.spark.sql.Dataset[String] = [value: string]\njsonValues: org.apache.spark.sql.DataFrame = [record: struct<id: string, ts: bigint ... 2 more fields>]\nsensorData: org.apache.spark.sql.Dataset[SensorData] = [id: string, ts: bigint ... 2 more fields]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 7,
      "time" : "Took: 1.634s, at 2018-11-14 10:05"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "F69903037F07494EA9A42815B386C0A4"
    },
    "cell_type" : "code",
    "source" : [ "sensorData.printSchema()" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "root\n |-- id: string (nullable = true)\n |-- ts: long (nullable = true)\n |-- temp: double (nullable = true)\n |-- hum: double (nullable = true)\n\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 8,
      "time" : "Took: 0.867s, at 2018-11-14 10:05"
    } ]
  }, {
    "metadata" : {
      "id" : "E579B7773A82414885636B662D189409"
    },
    "cell_type" : "markdown",
    "source" : "## Explore the data stream\nTo view the streaming data, we will use the `memory` sink and query the resulting table to get samples of the data."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "0A62B3731D884D1185F153937359EB80"
    },
    "cell_type" : "code",
    "source" : [ "val visualizationQuery = sensorData.writeStream\n", "  .queryName(\"visualization\")    // this query name will be the SQL table name\n", "  .outputMode(\"append\")\n", "  .format(\"memory\")\n", "  .start()" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "visualizationQuery: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@7d7b2519\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 9,
      "time" : "Took: 1.135s, at 2018-11-14 10:05"
    } ]
  }, {
    "metadata" : {
      "id" : "2E96BBE34C994DEA8CD5200022660825"
    },
    "cell_type" : "markdown",
    "source" : "## Explore the Data\nThe `memory` sink creates an in-memory SQL table (like a `tempTable`) that we can query using Spark SQL\nThe result of the query is a static `Dataframe` that contains a snapshot of the data."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "3A8498745D0E4A86BFB12ED10544529E"
    },
    "cell_type" : "code",
    "source" : [ "val sampleDataset = sparkSession.sql(\"select * from visualization\")\n" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "sampleDataset: org.apache.spark.sql.DataFrame = [id: string, ts: bigint ... 2 more fields]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 10,
      "time" : "Took: 0.885s, at 2018-11-14 10:05"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "1EED0786D37745B98B276F84B6661364"
    },
    "cell_type" : "code",
    "source" : [ "// This is a static Dataset!\n", "sampleDataset.isStreaming" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res14: Boolean = false\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "false"
      },
      "output_type" : "execute_result",
      "execution_count" : 11,
      "time" : "Took: 1.156s, at 2018-11-14 10:05"
    } ]
  }, {
    "metadata" : {
      "id" : "482CE35A1729444EBCAEC703D0D44EDA"
    },
    "cell_type" : "markdown",
    "source" : "### Our dataset is backed by the streaming data, it will update each time we execute an action, delivering the latest data."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "A7BE1142BA7F478DBD40DD167000E930"
    },
    "cell_type" : "code",
    "source" : [ "sampleDataset.count" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res16: Long = 88\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "88"
      },
      "output_type" : "execute_result",
      "execution_count" : 12,
      "time" : "Took: 1.708s, at 2018-11-14 10:05"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "6746B72D7F5D40EEA68EBBA2911BEF37"
    },
    "cell_type" : "code",
    "source" : [ "sampleDataset.count" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res18: Long = 144\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "144"
      },
      "output_type" : "execute_result",
      "execution_count" : 13,
      "time" : "Took: 1.098s, at 2018-11-14 10:05"
    } ]
  }, {
    "metadata" : {
      "id" : "0693F6BC6F6E422281CFDB057D95F707"
    },
    "cell_type" : "markdown",
    "source" : "## Visualize the Data\nWe will make a custom live update by querying the stream every so often for the latest updates"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "6D330DDECB47432C8B2C534C9E9291FF"
    },
    "cell_type" : "code",
    "source" : [ "val dummy = Seq((System.currentTimeMillis, 0.1), (System.currentTimeMillis, 0.1))\n", "\n", "val chart = CustomPlotlyChart(dummy,\n", "                  layout=s\"{title: 'sensor data sample'}\",\n", "                  dataOptions=\"\"\"{type: 'line'}\"\"\",\n", "                  dataSources=\"{x: '_1', y: '_2' }\")" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "dummy: Seq[(Long, Double)] = List((1542186313065,0.1), (1542186313066,0.1))\nchart: notebook.front.widgets.charts.CustomPlotlyChart[Seq[(Long, Double)]] = <CustomPlotlyChart widget>\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 14,
      "time" : "Took: 1.413s, at 2018-11-14 10:05"
    } ]
  }, {
    "metadata" : {
      "id" : "C96D62E1D7754A2F8C9CCCB23C2D4E29"
    },
    "cell_type" : "markdown",
    "source" : "## Async update of our visualization\nWe will use a plain old Thread to run a recurrent query on our in-memory table and update the chart accordingly.\n"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "D496C339A4F24B9F8038D2DA970220E5"
    },
    "cell_type" : "code",
    "source" : [ "@volatile var running = true" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "running: Boolean = true\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 15,
      "time" : "Took: 0.700s, at 2018-11-14 10:05"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "645C005ED2D843EF81E0A96329F9C7D4"
    },
    "cell_type" : "code",
    "source" : [ "import scala.concurrent.duration._\n", "import scala.annotation.tailrec\n", "\n", "val updater = new Thread() {\n", "  @tailrec\n", "  def visualize(): Unit = {\n", "    val lastMinute = System.currentTimeMillis - 1.minute.toMillis\n", "    val data = sampleDataset.where($\"ts\" > lastMinute and $\"id\" === \"devoxx\").as[SensorData]\n", "                            .map{case SensorData(id, ts, value, level) => (ts/1000%3600, value)}.collect\n", "    chart.applyOn(data)\n", "    if (running) {\n", "      Thread.sleep(1.second.toMillis)\n", "      visualize()\n", "    } else ()\n", "  } \n", "  \n", "  override def run() {\n", "    visualize()\n", "  }\n", "}.start()\n" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import scala.concurrent.duration._\nimport scala.annotation.tailrec\nupdater: Unit = ()\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 16,
      "time" : "Took: 1.487s, at 2018-11-14 10:05"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "FDF98A5CE00E4CAD89A9BEA802A0C16F"
    },
    "cell_type" : "code",
    "source" : [ "chart" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res23: notebook.front.widgets.charts.CustomPlotlyChart[Seq[(Long, Double)]] = <CustomPlotlyChart widget>\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "<div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anon144bf930e7b0cee333ebcff4dac4156e&quot;,&quot;dataInit&quot;:[{&quot;_1&quot;:1542186313065,&quot;_2&quot;:0.1},{&quot;_1&quot;:1542186313066,&quot;_2&quot;:0.1}],&quot;genId&quot;:&quot;387949690&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/customPlotlyChart'], \n      function(playground, _magiccustomPlotlyChart) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magiccustomPlotlyChart,\n    \"o\": {\"js\":\"var layout = {title: 'sensor data sample'}; var dataSources={x: '_1', y: '_2' }; var dataOptions = {type: 'line'}; var extraOptions = {}\",\"headers\":[\"_1\",\"_2\"],\"height\":400}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    <div>\n      <span class=\"chart-total-item-count\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anonf6c2e06d27a3f5c8e3b3ffd18818a6ae&quot;,&quot;initialValue&quot;:&quot;2&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p> entries total</span>\n      <span class=\"chart-sampling-warning\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anon0a9abbf8ef8751eec965f24543cad9bd&quot;,&quot;initialValue&quot;:&quot;&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p></span>\n      <div>\n      </div>\n    </div></div>"
      },
      "output_type" : "execute_result",
      "execution_count" : 17,
      "time" : "Took: 1.194s, at 2018-11-14 10:05"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "5F80EB02EB934828854A917DBF7D0DE1"
    },
    "cell_type" : "code",
    "source" : [ "// visualizationQuery.stop()\n", "// running = false" ],
    "outputs" : [ {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 18,
      "time" : "Took: 1.129s, at 2018-11-14 10:05"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "A8292D947D7E4A889B416F5C2BB158C7"
    },
    "cell_type" : "markdown",
    "source" : "# Improve the data with sliding windows"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "BD00015ED0564F9B9F405737792E7743"
    },
    "cell_type" : "code",
    "source" : [ "import org.apache.spark.sql.types._" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import org.apache.spark.sql.types._\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 19,
      "time" : "Took: 0.816s, at 2018-11-14 10:05"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "BBDA2F693B6247DD8849298DA0BC769B"
    },
    "cell_type" : "code",
    "source" : [ "val toSeconds = udf((ts:Long) => ts/1000)\n", "val sensorMovingAverage = sensorData.withColumn(\"timestamp\", toSeconds($\"ts\").cast(TimestampType))\n", "                                          .withWatermark(\"timestamp\", \"30 seconds\")\n", "                                          .groupBy($\"id\", window($\"timestamp\", \"30 seconds\", \"10 seconds\"))\n", "                                          .agg(avg($\"temp\"))" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "toSeconds: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,LongType,Some(List(LongType)))\nsensorMovingAverage: org.apache.spark.sql.DataFrame = [id: string, window: struct<start: timestamp, end: timestamp> ... 1 more field]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 20,
      "time" : "Took: 1.269s, at 2018-11-14 10:05"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "9BF82601363C40579E8DBC9AF9AC2F89"
    },
    "cell_type" : "code",
    "source" : [ "sensorMovingAverage.printSchema" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "root\n |-- id: string (nullable = true)\n |-- window: struct (nullable = true)\n |    |-- start: timestamp (nullable = true)\n |    |-- end: timestamp (nullable = true)\n |-- avg(temp): double (nullable = true)\n\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 21,
      "time" : "Took: 1.249s, at 2018-11-14 10:05"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "414735A902F34CDE84D1C3642D512973"
    },
    "cell_type" : "code",
    "source" : [ "val windowedSensorQuery = sensorMovingAverage.writeStream\n", "  .queryName(\"movingAverage\")    // this query name will be the table name\n", "  .outputMode(\"append\")  \n", "  .format(\"memory\")\n", "  .start()" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "windowedSensorQuery: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@7917a017\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 22,
      "time" : "Took: 0.914s, at 2018-11-14 10:05"
    } ]
  }, {
    "metadata" : {
      "id" : "DF8FC71EEC3142C6852A411BF1C54DBC"
    },
    "cell_type" : "markdown",
    "source" : "### Get the data from the in-memory table"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "89EE4EF3EFA845D987484FE216C865A3"
    },
    "cell_type" : "code",
    "source" : [ "val movingAvgDF = sparkSession.sql(\"select * from movingAverage\")" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "<console>:121: error: type mismatch;\n found   : $iw\n required: $iw\nval $iw = new $iw\n    ^\n"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "2E71B78A71D543358203125FE62FE6F5"
    },
    "cell_type" : "code",
    "source" : [ "movingAvgDF" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "<console>:70: error: not found: value movingAvgDF\n       movingAvgDF\n       ^\n"
    } ]
  }, {
    "metadata" : {
      "id" : "32708913EB914EAB8268E52A0F69296F"
    },
    "cell_type" : "markdown",
    "source" : "### Chart the Moving Average Data"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "AA5DF0B0AD6D4EF093A9879DB10A106E"
    },
    "cell_type" : "code",
    "source" : [ "import org.apache.spark.sql.functions._\n", "val lastMinute: Long = System.currentTimeMillis/1000 - 5.minute.toSeconds\n", "val mAvgSample = movingAvgDF.select($\"window.start\".cast(LongType) as \"timestamp\", $\"avg(temp)\" as \"temp\")\n", "                   .where($\"timestamp\" > lastMinute and $\"id\" === \"devoxx\")\n", "                   .orderBy($\"timestamp\")\n", "                   .as[(Long, Double)]\n", "                   .collect().map{case (ts, v) => (ts  % 3600,v)}\n", "\n", "\n", "CustomPlotlyChart(mAvgSample,\n", "                  layout=s\"{title: 'moving average sensor data'}\",\n", "                  dataOptions=\"\"\"{type: 'line'}\"\"\",\n", "                  dataSources=\"{x: '_1', y: '_2'}\")" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "<console>:70: error: not found: value movingAvgDF\nval mAvgSample = movingAvgDF.select($\"window.start\".cast(LongType) as \"timestamp\", $\"avg(temp)\" as \"temp\")\n                 ^\n<console>:74: error: value % is not a member of Any\n                   .collect().map{case (ts, v) => (ts  % 3600,v)}\n                                                       ^\n"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "B39288CF67F04535870AE968E0700074"
    },
    "cell_type" : "code",
    "source" : [ "// stop the ancilliary visualization queries\n", "windowedSensorQuery.stop()\n", "visualizationQuery.stop()\n", "running = false" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "running: Boolean = false\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 26,
      "time" : "Took: 0.852s, at 2018-11-14 10:05"
    } ]
  }, {
    "metadata" : {
      "id" : "495EC81428014BC4AE7CD9DE863EFB06"
    },
    "cell_type" : "markdown",
    "source" : "## Write our moving average data to our `sensor-clean` topic"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "707C86245D9542F384FFC006900AA88D"
    },
    "cell_type" : "code",
    "source" : [ "// First we prepare the schema to comply with the (key, value) model of Kafka\n", "val kafkaFormat = sensorMovingAverage\n", ".select($\"id\", $\"window.start\".cast(LongType) as \"timestamp\", $\"avg(temp)\" as \"temp\")\n", ".select($\"id\" as \"key\", to_json(struct($\"id\", $\"timestamp\", $\"temp\")) as \"value\")" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "kafkaFormat: org.apache.spark.sql.DataFrame = [key: string, value: string]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 27,
      "time" : "Took: 0.702s, at 2018-11-14 10:05"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "AF7660F21370444A89C197C94930052E"
    },
    "cell_type" : "code",
    "source" : [ "val kafkaWriterQuery = kafkaFormat.writeStream\n", "  .queryName(\"kafkaWriter\") \n", "  .outputMode(\"append\") \n", "  .format(\"kafka\")\n", "  .option(\"kafka.bootstrap.servers\", kafkaBootstrapServer)\n", "  .option(\"topic\", targetTopic)\n", "  .option(\"checkpointLocation\", \"/tmp/spark/checkpoint4\")\n", "  .option(\"failOnDataLoss\", \"false\")\n", "  .start()" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "kafkaWriterQuery: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@51b8963a\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 28,
      "time" : "Took: 0.783s, at 2018-11-14 10:05"
    } ]
  }, {
    "metadata" : {
      "id" : "C1A1F325212A4601910959A098B6E22A"
    },
    "cell_type" : "markdown",
    "source" : "## View Progress"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab1300593006-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "F359A73CFAF04B978876322746ECF620"
    },
    "cell_type" : "code",
    "source" : [ "val progress = kafkaWriterQuery.recentProgress" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "progress: Array[org.apache.spark.sql.streaming.StreamingQueryProgress] =\nArray({\n  \"id\" : \"34e7f374-6a2f-4b02-992b-eb329d52ed45\",\n  \"runId\" : \"26ffdf13-b583-48fd-90f0-d507e51f16ab\",\n  \"name\" : \"kafkaWriter\",\n  \"timestamp\" : \"2018-11-14T09:05:26.155Z\",\n  \"batchId\" : 2218,\n  \"numInputRows\" : 63,\n  \"processedRowsPerSecond\" : 14.02493321460374,\n  \"durationMs\" : {\n    \"addBatch\" : 4390,\n    \"getBatch\" : 9,\n    \"queryPlanning\" : 62,\n    \"triggerExecution\" : 4492\n  },\n  \"eventTime\" : {\n    \"avg\" : \"2018-11-14T00:24:13.666Z\",\n    \"max\" : \"2018-11-14T00:24:15.000Z\",\n    \"min\" : \"2018-11-14T00:24:13.000Z\",\n    \"watermark\" : \"2018-11-14T00:23:43.000Z\"\n  },\n  \"stateOperators\" : [ {\n    \"numRowsTotal\" : 4518,\n    \"numRowsUpdated\" : 186,\n    \"memoryUsedBytes\" : 1323799\n  } ],\n  \"sources\" : [ {\n    \"d..."
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 32,
      "time" : "Took: 0.920s, at 2018-11-14 10:06"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab1337165318-1\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "06AFEE5AE0764ECCB6F01239CF4267B9"
    },
    "cell_type" : "code",
    "source" : [ "progress.map(entry  => (entry.inputRowsPerSecond, entry.processedRowsPerSecond))" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res41: Array[(Double, Double)] = Array()\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "empty array"
      },
      "output_type" : "execute_result",
      "execution_count" : 30,
      "time" : "Took: 1.446s, at 2018-11-14 10:05"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "1EBD79F917FA447F80703F4717B1FC83"
    },
    "cell_type" : "code",
    "source" : [ "" ],
    "outputs" : [ {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 31,
      "time" : "Took: 1.215s, at 2018-11-14 10:05"
    } ]
  } ],
  "nbformat" : 4
}