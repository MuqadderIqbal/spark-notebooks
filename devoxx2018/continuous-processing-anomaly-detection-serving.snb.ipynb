{
  "metadata" : {
    "id" : "680a18bd-5fbf-45e7-aafb-92d31a2c54ca",
    "name" : "continuous-processing-anomaly-detection-serving",
    "user_save_timestamp" : "1969-12-31T19:00:00.000Z",
    "auto_save_timestamp" : "1969-12-31T19:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "sparkNotebook" : null,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : [ "org.apache.spark %% spark-sql-kafka-0-10 % 2.3.0", "org.apache.spark %% spark-streaming-kafka-0-8 % 2.3.0" ],
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : null,
    "customVars" : null
  },
  "cells" : [ {
    "metadata" : {
      "id" : "CE89CE47AA7D417581D75EA6E34A6CFD"
    },
    "cell_type" : "markdown",
    "source" : "#Real-Time Anomaly Detection Using Continuous Processing\nThis notebook uses the exported M2 Model by Spark Streaming and combines it with a Continuous Processing job in Structured Streaming to deliver real-time anomaly detection on the raw data stream."
  }, {
    "metadata" : {
      "id" : "92D6258DD425441388775107922BAF05"
    },
    "cell_type" : "markdown",
    "source" : "## Common settings"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "F978814AB1FF425BA09BFDE662F022C6"
    },
    "cell_type" : "code",
    "source" : [ "import org.apache.spark.streaming.Duration" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import org.apache.spark.streaming.Duration\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 1,
      "time" : "Took: 0.897s, at 2018-10-27 05:35"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "7868577A32F048309DE0AD504C46624C"
    },
    "cell_type" : "code",
    "source" : [ "val topic = \"sensor-raw\"\n", "val modelTopic = \"modelTopic\"\n", "val anomalyTopic = \"anomalyTopic\"\n", "val kafkaBootstrapServer = \"172.17.0.2:9092\"\n", "val threshold = 4.0\n", "val targetDir = \"/tmp/anomaly/model\"\n", "val interval = Duration(30000)" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "topic: String = sensor-raw\nmodelTopic: String = modelTopic\nkafkaBootstrapServer: String = 172.17.0.2:9092\nthreshold: Double = 4.0\ntargetDir: String = /tmp/anomaly/model\ninterval: org.apache.spark.streaming.Duration = 30000 ms\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 2,
      "time" : "Took: 0.985s, at 2018-10-27 05:36"
    } ]
  }, {
    "metadata" : {
      "id" : "9391BBE443D0446E89FCA4FA1E27B6B5"
    },
    "cell_type" : "markdown",
    "source" : "## Case class and Schema definitions"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "4E05EEA1D7A742DA9DABBF7832C645AB"
    },
    "cell_type" : "code",
    "source" : [ "case class M2(n:Int, mean: Double, m2:Double) {\n", "  def variance: Option[Double] = {\n", "    if (n<2) None else Some(m2/(n-1))\n", "  }\n", "  def stdev: Option[Double] = variance.map(Math.sqrt)\n", "}\n", "case class IdM2(id:String, m2: M2)\n", "case class SensorData(id: String, ts: Long, temp: Double, hum: Double)" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "defined class M2\ndefined class IdM2\ndefined class SensorData\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 2,
      "time" : "Took: 1.882s, at 2018-10-27 05:31"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "31FBD06486E649468601F19E237E1375"
    },
    "cell_type" : "code",
    "source" : [ "import org.apache.spark.sql.Encoders\n", "val idM2Schema = Encoders.product[IdM2].schema\n", "val sensorSchema = Encoders.product[SensorData].schema" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import org.apache.spark.sql.Encoders\nidM2Schema: org.apache.spark.sql.types.StructType = StructType(StructField(id,StringType,true), StructField(m2,StructType(StructField(n,IntegerType,false), StructField(mean,DoubleType,false), StructField(m2,DoubleType,false)),true))\nsensorSchema: org.apache.spark.sql.types.StructType = StructType(StructField(id,StringType,true), StructField(ts,LongType,false), StructField(temp,DoubleType,false), StructField(hum,DoubleType,false))\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 3,
      "time" : "Took: 2.652s, at 2018-10-27 05:31"
    } ]
  }, {
    "metadata" : {
      "id" : "D31AABFB4BA644A2A52632A97B95DCB9"
    },
    "cell_type" : "markdown",
    "source" : "## Read the model Stream from Kafka using Spark Streaming"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "70BB9604C6214CE68086EF1BCED2A368"
    },
    "cell_type" : "code",
    "source" : [ "import org.apache.spark.streaming.StreamingContext\n", "@transient val streamingContext = new StreamingContext(sparkSession.sparkContext, interval)" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import org.apache.spark.streaming.StreamingContext\nstreamingContext: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@4416af39\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 3,
      "time" : "Took: 1.177s, at 2018-10-27 05:36"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "6B9F8A8A791E40B58418DDCCE8FF2468"
    },
    "cell_type" : "code",
    "source" : [ "import org.apache.kafka.clients.consumer.ConsumerRecord\n", "import kafka.serializer.StringDecoder\n", "import org.apache.spark.streaming.kafka._\n", "\n", "val kafkaParams = Map[String, String](\n", "  \"metadata.broker.list\" -> kafkaBootstrapServer,\n", "  \"group.id\" -> \"model-serving-group\",\n", "  \"auto.offset.reset\" -> \"largest\"\n", ")\n", "\n", "val topics = Set(modelTopic)\n", "@transient val modelStream = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](\n", "     streamingContext, kafkaParams, topics)" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "<console>:95: warning: object KafkaUtils in package kafka is deprecated: Update to Kafka 0.10 integration\n       @transient val modelStream = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](\n                                    ^\nimport org.apache.kafka.clients.consumer.ConsumerRecord\nimport kafka.serializer.StringDecoder\nimport org.apache.spark.streaming.kafka._\nkafkaParams: scala.collection.immutable.Map[String,String] = Map(metadata.broker.list -> 172.17.0.2:9092, group.id -> model-serving-group, auto.offset.reset -> largest)\ntopics: scala.collection.immutable.Set[String] = Set(modelTopic)\nmodelStream: org.apache.spark.streaming.dstream.InputDStream[(String, String)] = org.apache.spark.streaming.kafka.DirectKafkaInputDStream@53f2ab73\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 2,
      "time" : "Took: 1.224s, at 2018-10-27 05:55"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "0B80CCD060AE42E78F4DC47572CB9317"
    },
    "cell_type" : "code",
    "source" : [ "var query: org.apache.spark.sql.DataFrame = _" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "rawData: org.apache.spark.sql.DataFrame = null\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 1,
      "time" : "Took: 0.949s, at 2018-10-27 06:02"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "255A91A2EA3249B0B69D5CC7AD632818"
    },
    "cell_type" : "code",
    "source" : [ "import org.apache.spark.sql.functions._\n", "modelStream.foreachRDD{ rdd =>\n", "  if (rdd.nonEmpty) {\n", "    val models = rdd.map{case (k,v) => v}.toDF(\"value\")\n", "    val mostRecentM2JsonModel = models.select(from_json($\"value\", idM2Schema)).as[IdM2]\n", "    val m2Map = mostRecentM2JsonModel.collect.map(idM2=> (idM2.id, idM2.m2)).toMap\n", "    \n", "    if (query != null) {\n", "      query.stop()\n", "    }\n", "    val rawData = sparkSession.readStream\n", "      .format(\"kafka\")\n", "      .option(\"kafka.bootstrap.servers\", kafkaBootstrapServer)\n", "      .option(\"subscribe\", topic)\n", "      .option(\"checkpointDir\", \"/tmp/checkpoint\")\n", "      .option(\"startingOffsets\", \"earliest\")\n", "      .load()\n", "    val rawValues = rawData.selectExpr(\"CAST(value AS STRING)\").as[String]\n", "    val jsonValues = rawValues.select(from_json($\"value\", sensorSchema) as \"record\")\n", "    val sensorData = jsonValues.select(\"record.*\").as[SensorData]\n", "    val scoreStream = sensorData.flatMap{case SensorData(id, ts, temp, hum) => \n", "                                     val m2Opt = m2Map.get(id)\n", "                                     m2Opt.map{m2 => (id, ts, temp, m2.mean, m2.stdev)}\n", "                                    }.toDF(\"id\", \"ts\",\"temp\",\"mean\",\"std\")\n", "    \n", "    val anomalies = scoreStream.where($\"temp\" > $\"mean\"+$\"std\"*threshold or $\"temp\" < $\"mean\"-$\"std\"*threshold)\n", "    import org.apache.spark.sql.streaming.Trigger\n", "    val query = anomalies.writeStream\n", "      .format(\"kafka\")\n", "      .queryName(\"continuousStreamDetection\")\n", "      .trigger(Trigger.Continuous(\"1 second\"))\n", "      .outputMode(\"append\") \n", "      .option(\"kafka.bootstrap.servers\", kafkaBootstrapServer)\n", "      .option(\"topic\", anomalyTopic)\n", "      .option(\"checkpointLocation\", \"/tmp/spark/checkpoint-a1\")\n", "      .option(\"failOnDataLoss\", \"false\")\n", "    .start()\n", "  }\n", "}\n", "\n", "    " ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "D329053A96304C78B525007B9F6D5456"
    },
    "cell_type" : "code",
    "source" : [ "streamingContext.start()" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "BBB21A0011E14F7F89B017D8BAAD0CC4"
    },
    "cell_type" : "markdown",
    "source" : "## Read the Anomaly Stream Back From Kafka"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "D681695818F146B79C827C8081A6ED4D"
    },
    "cell_type" : "code",
    "source" : [ "val rawData = sparkSession.readStream\n", "      .format(\"kafka\")\n", "      .option(\"kafka.bootstrap.servers\", kafkaBootstrapServer)\n", "      .option(\"subscribe\", anomalyTopic)\n", "      .option(\"startingOffsets\", \"latest\")\n", "      .load()" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "rawData: org.apache.spark.sql.DataFrame = [key: binary, value: binary ... 5 more fields]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 7,
      "time" : "Took: 1.513s, at 2018-10-23 20:53"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "9B15466240D1494C871CE30606A8CBA5"
    },
    "cell_type" : "code",
    "source" : [ "case class AnomalyReport(id: String, ts: Long, temp: Double,mean: Double, std:Double)" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "39E1CF9668D343819F3C979874A6B2D8"
    },
    "cell_type" : "code",
    "source" : [ "val rawValues = rawData.selectExpr(\"CAST(value AS STRING)\").as[String]\n", "val jsonValues = rawValues.select(from_json($\"value\", sensorSchema) as \"record\")\n", "val sensorData = jsonValues.select(\"record.*\").as[SensorData]" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "rawValues: org.apache.spark.sql.Dataset[String] = [value: string]\njsonValues: org.apache.spark.sql.DataFrame = [record: struct<id: string, ts: bigint ... 2 more fields>]\nsensorData: org.apache.spark.sql.Dataset[SensorData] = [id: string, ts: bigint ... 2 more fields]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 8,
      "time" : "Took: 1.853s, at 2018-10-23 20:53"
    } ]
  }, {
    "metadata" : {
      "id" : "F29123DAD6014DBA9294918964B88A9D"
    },
    "cell_type" : "markdown",
    "source" : "## ----- We have already seen a similar process. Here comes the interesting part -----"
  }, {
    "metadata" : {
      "id" : "A5FAB1BBB48045338CE02A807B218BF9"
    },
    "cell_type" : "markdown",
    "source" : "## Augment the data with its estimated mean an stdev"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "F3C88CAC1F604D8081012082ECB28122"
    },
    "cell_type" : "code",
    "source" : [ "val scoreStream = sensorData.flatMap{case SensorData(id, ts, temp, hum) => \n", "                                     val m2Opt = m2Map.get(id)\n", "                                     m2Opt.map{m2 => (id, ts, temp, m2.mean, m2.stdev)}\n", "                                    }.toDF(\"id\", \"ts\",\"temp\",\"mean\",\"std\")\n", "                                 " ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "scoreStream: org.apache.spark.sql.DataFrame = [id: string, ts: bigint ... 3 more fields]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 9,
      "time" : "Took: 1.206s, at 2018-10-23 20:54"
    } ]
  }, {
    "metadata" : {
      "id" : "B462BA37CDA949C886B3C9ED22A269AB"
    },
    "cell_type" : "markdown",
    "source" : "## Filter out outliers suspected to be anomalies"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "3D92FF17C43B425E90BABFD505358967"
    },
    "cell_type" : "code",
    "source" : [ "import org.apache.spark.sql.functions._\n", "val suspects = scoreStream.where($\"temp\" > $\"mean\"+$\"std\"*threshold or $\"temp\" < $\"mean\"-$\"std\"*threshold)" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import org.apache.spark.sql.functions._\nsuspects: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: string, ts: bigint ... 3 more fields]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 10,
      "time" : "Took: 1.084s, at 2018-10-23 20:54"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "21245E343B1D42A1B7675D4B66B99B0C"
    },
    "cell_type" : "code",
    "source" : [ "import org.apache.spark.sql.streaming.Trigger\n", "val query = suspects.writeStream\n", "                    .format(\"memory\")\n", "                    .queryName(\"continuousStreamDetection\")\n", "                    .trigger(Trigger.Continuous(\"1 second\"))\n", "                    .start() " ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import org.apache.spark.sql.streaming.Trigger\nquery: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@5be2d995\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 11,
      "time" : "Took: 1.433s, at 2018-10-23 20:54"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "28B9D372DC66468B976A4CBF3B02BEF2"
    },
    "cell_type" : "code",
    "source" : [ "//query.stop" ],
    "outputs" : [ {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 12,
      "time" : "Took: 1.267s, at 2018-10-23 20:54"
    } ]
  }, {
    "metadata" : {
      "id" : "4CC61893D0C44D1FAAA3BCFF019426E3"
    },
    "cell_type" : "markdown",
    "source" : "## Let's do some Visualization"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "9DE102EBB974485EB8C014F85A14AC7A"
    },
    "cell_type" : "code",
    "source" : [ "val dummy = Seq((\"id\", 0.0))\n", "\n", "val chart = CustomPlotlyChart(dummy,\n", "                  layout=s\"{title: 'sensor anomaly indicator'}\",\n", "                  dataOptions=\"\"\"{type: 'bar'}\"\"\",\n", "                  dataSources=\"{x: '_1', y: '_2' }\")\n", "chart" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "dummy: Seq[(String, Double)] = List((id,0.0))\nchart: notebook.front.widgets.charts.CustomPlotlyChart[Seq[(String, Double)]] = <CustomPlotlyChart widget>\nres15: notebook.front.widgets.charts.CustomPlotlyChart[Seq[(String, Double)]] = <CustomPlotlyChart widget>\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "<div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anon07c0b5390a70e1b093e4236874b6ee8c&quot;,&quot;dataInit&quot;:[{&quot;_1&quot;:&quot;id&quot;,&quot;_2&quot;:0}],&quot;genId&quot;:&quot;315933529&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/customPlotlyChart'], \n      function(playground, _magiccustomPlotlyChart) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magiccustomPlotlyChart,\n    \"o\": {\"js\":\"var layout = {title: 'sensor anomaly indicator'}; var dataSources={x: '_1', y: '_2' }; var dataOptions = {type: 'bar'}; var extraOptions = {}\",\"headers\":[\"_1\",\"_2\"],\"height\":400}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    <div>\n      <span class=\"chart-total-item-count\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anon679219044a12c7091f51ce7e8dcc75e2&quot;,&quot;initialValue&quot;:&quot;1&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p> entries total</span>\n      <span class=\"chart-sampling-warning\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anon169742389f3b9115d2c942aa65196ffd&quot;,&quot;initialValue&quot;:&quot;&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p></span>\n      <div>\n      </div>\n    </div></div>"
      },
      "output_type" : "execute_result",
      "execution_count" : 13,
      "time" : "Took: 1.833s, at 2018-10-23 20:54"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "C2AB56A90497432E8CA7F54643BC64E3"
    },
    "cell_type" : "code",
    "source" : [ "@volatile var running = true" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "running: Boolean = true\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 14,
      "time" : "Took: 0.854s, at 2018-10-23 20:54"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "2370A21BFEB449AA8E179F1711F65F7A"
    },
    "cell_type" : "code",
    "source" : [ "import scala.concurrent.duration._\n", "import scala.annotation.tailrec\n", "\n", "val updater = new Thread() {\n", "  @tailrec\n", "  def visualize(): Unit = {\n", "    val currentTimeThreshold = System.currentTimeMillis - 5*1000\n", "    val data = sparkSession.sql(s\"select id, temp from continuousStreamDetection where ts > $currentTimeThreshold\")\n", "                           .as[(String, Double)]\n", "                           .collect\n", "    if (data.nonEmpty) chart.applyOn(data)\n", "    if (running) {\n", "      Thread.sleep(1.second.toMillis)\n", "      visualize()\n", "    } else ()\n", "  } \n", "  \n", "  override def run() {\n", "    visualize()\n", "  }\n", "}.start()\n" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import scala.concurrent.duration._\nimport scala.annotation.tailrec\nupdater: Unit = ()\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 15,
      "time" : "Took: 1.392s, at 2018-10-23 20:54"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "D38B852520F944CE819CF0B23F85BD80"
    },
    "cell_type" : "code",
    "source" : [ "--" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "949C3CC7C314434984FCF83588819EE5"
    },
    "cell_type" : "code",
    "source" : [ "// execute to stop the chart updating thread\n", "running = false" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "running: Boolean = false\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 16,
      "time" : "Took: 1.092s, at 2018-10-23 20:55"
    } ]
  }, {
    "metadata" : {
      "id" : "73CC98AACACD41848CC0F78478C5C03B"
    },
    "cell_type" : "markdown",
    "source" : "# -- o --"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab2113607371-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "76A730A3B6DA45B58D350EBE86C260EF"
    },
    "cell_type" : "code",
    "source" : [ "memTable.where($\"id\" === \"office\")" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res150: org.apache.spark.sql.Dataset[(String, Long, Double)] = [id: string, ts: bigint ... 1 more field]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "<div class=\"df-canvas\">\n      <script data-this=\"{&quot;dataId&quot;:&quot;anon647819b1833202a2f70aeb0d3396743a&quot;,&quot;partitionIndexId&quot;:&quot;anon7291e2f670ae487b2e46f91e98d62005&quot;,&quot;numPartitions&quot;:9,&quot;dfSchema&quot;:{&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[{&quot;name&quot;:&quot;id&quot;,&quot;type&quot;:&quot;string&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;ts&quot;,&quot;type&quot;:&quot;long&quot;,&quot;nullable&quot;:false,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;temp&quot;,&quot;type&quot;:&quot;double&quot;,&quot;nullable&quot;:false,&quot;metadata&quot;:{}}]}}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/dataframe','../javascripts/notebook/consoleDir'], \n      function(dataframe, extension) {\n        dataframe.call(data, this, extension);\n      }\n    );/*]]>*/</script>\n      <link rel=\"stylesheet\" href=\"/assets/stylesheets/ipython/css/dataframe.css\" type=\"text/css\"/>\n    </div>"
      },
      "output_type" : "execute_result",
      "execution_count" : 93,
      "time" : "Took: 1.653s, at 2018-09-12 00:40"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "8C79874D846F4EA8849475E6C16BA620"
    },
    "cell_type" : "code",
    "source" : [ "" ],
    "outputs" : [ ]
  } ],
  "nbformat" : 4
}