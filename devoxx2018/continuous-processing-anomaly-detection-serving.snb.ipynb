{
  "metadata" : {
    "id" : "680a18bd-5fbf-45e7-aafb-92d31a2c54ca",
    "name" : "continuous-processing-anomaly-detection-serving",
    "user_save_timestamp" : "1969-12-31T19:00:00.000Z",
    "auto_save_timestamp" : "1969-12-31T19:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "sparkNotebook" : null,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : [ "org.apache.spark %% spark-sql-kafka-0-10 % 2.3.0", "org.apache.spark %% spark-streaming-kafka-0-8 % 2.3.0" ],
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : {
      "spark.sql.codegen.wholeStage" : "false"
    },
    "customVars" : null
  },
  "cells" : [ {
    "metadata" : {
      "id" : "CE89CE47AA7D417581D75EA6E34A6CFD"
    },
    "cell_type" : "markdown",
    "source" : "#Real-Time Anomaly Detection Using Continuous Processing\nThis notebook uses the exported M2 Model by Spark Streaming and combines it with a Continuous Processing job in Structured Streaming to deliver real-time anomaly detection on the raw data stream."
  }, {
    "metadata" : {
      "id" : "92D6258DD425441388775107922BAF05"
    },
    "cell_type" : "markdown",
    "source" : "## Common settings"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "F978814AB1FF425BA09BFDE662F022C6"
    },
    "cell_type" : "code",
    "source" : [ "import org.apache.spark.streaming.Duration" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import org.apache.spark.streaming.Duration\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 1,
      "time" : "Took: 1.331s, at 2018-11-14 13:31"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "7868577A32F048309DE0AD504C46624C"
    },
    "cell_type" : "code",
    "source" : [ "val topic = \"sensor-raw\"\n", "val modelTopic = \"modelTopic\"\n", "val anomalyTopic = \"anomalyTopic\"\n", "val kafkaBootstrapServer = \"172.17.0.2:9092\"\n", "val threshold = 4.0\n", "val targetDir = \"/tmp/anomaly/model\"\n", "val modelRefreshInterval = Duration(30000)" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "topic: String = sensor-raw\nmodelTopic: String = modelTopic\nanomalyTopic: String = anomalyTopic\nkafkaBootstrapServer: String = 172.17.0.2:9092\nthreshold: Double = 4.0\ntargetDir: String = /tmp/anomaly/model\nmodelRefreshInterval: org.apache.spark.streaming.Duration = 30000 ms\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 2,
      "time" : "Took: 1.287s, at 2018-11-14 13:31"
    } ]
  }, {
    "metadata" : {
      "id" : "9391BBE443D0446E89FCA4FA1E27B6B5"
    },
    "cell_type" : "markdown",
    "source" : "## Case class and Schema definitions\n(we have seen this already)"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "4E05EEA1D7A742DA9DABBF7832C645AB"
    },
    "cell_type" : "code",
    "source" : [ "case class M2(n:Int, mean: Double, m2:Double) {\n", "  def variance: Option[Double] = {\n", "    if (n<2) None else Some(m2/(n-1))\n", "  }\n", "  def stdev: Option[Double] = variance.map(Math.sqrt)\n", "}\n", "case class IdM2(id:String, m2: M2)\n", "case class SensorData(id: String, ts: Long, temp: Double, hum: Double)" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "defined class M2\ndefined class IdM2\ndefined class SensorData\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 3,
      "time" : "Took: 1.615s, at 2018-11-14 13:31"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "31FBD06486E649468601F19E237E1375"
    },
    "cell_type" : "code",
    "source" : [ "import org.apache.spark.sql.Encoders\n", "val idM2Schema = Encoders.product[IdM2].schema\n", "val sensorSchema = Encoders.product[SensorData].schema" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import org.apache.spark.sql.Encoders\nidM2Schema: org.apache.spark.sql.types.StructType = StructType(StructField(id,StringType,true), StructField(m2,StructType(StructField(n,IntegerType,false), StructField(mean,DoubleType,false), StructField(m2,DoubleType,false)),true))\nsensorSchema: org.apache.spark.sql.types.StructType = StructType(StructField(id,StringType,true), StructField(ts,LongType,false), StructField(temp,DoubleType,false), StructField(hum,DoubleType,false))\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 4,
      "time" : "Took: 2.481s, at 2018-11-14 13:31"
    } ]
  }, {
    "metadata" : {
      "id" : "D31AABFB4BA644A2A52632A97B95DCB9"
    },
    "cell_type" : "markdown",
    "source" : "## Read the model Stream using Spark Streaming"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "70BB9604C6214CE68086EF1BCED2A368"
    },
    "cell_type" : "code",
    "source" : [ "import org.apache.spark.streaming.StreamingContext\n", "@transient val streamingContext = new StreamingContext(sparkSession.sparkContext, modelRefreshInterval)" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import org.apache.spark.streaming.StreamingContext\nstreamingContext: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@2b97d30a\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 5,
      "time" : "Took: 1.285s, at 2018-11-14 13:31"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "6B9F8A8A791E40B58418DDCCE8FF2468"
    },
    "cell_type" : "code",
    "source" : [ "import org.apache.kafka.clients.consumer.ConsumerRecord\n", "import kafka.serializer.StringDecoder\n", "import org.apache.spark.streaming.kafka._\n", "\n", "val kafkaParams = Map[String, String](\n", "  \"metadata.broker.list\" -> kafkaBootstrapServer,\n", "  \"group.id\" -> \"model-serving-group\",\n", "  \"auto.offset.reset\" -> \"largest\"\n", ")\n", "\n", "val topics = Set(modelTopic)\n", "@transient val modelStream = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](\n", "     streamingContext, kafkaParams, topics)" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "<console>:79: warning: object KafkaUtils in package kafka is deprecated: Update to Kafka 0.10 integration\n       @transient val modelStream = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](\n                                    ^\nimport org.apache.kafka.clients.consumer.ConsumerRecord\nimport kafka.serializer.StringDecoder\nimport org.apache.spark.streaming.kafka._\nkafkaParams: scala.collection.immutable.Map[String,String] = Map(metadata.broker.list -> 172.17.0.2:9092, group.id -> model-serving-group, auto.offset.reset -> largest)\ntopics: scala.collection.immutable.Set[String] = Set(modelTopic)\nmodelStream: org.apache.spark.streaming.dstream.InputDStream[(String, String)] = org.apache.spark.streaming.kafka.DirectKafkaInputDStream@3541ab5c\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 6,
      "time" : "Took: 2.247s, at 2018-11-14 13:31"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "0B80CCD060AE42E78F4DC47572CB9317"
    },
    "cell_type" : "code",
    "source" : [ "var query: org.apache.spark.sql.streaming.StreamingQuery = _" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "query: org.apache.spark.sql.streaming.StreamingQuery = null\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 7,
      "time" : "Took: 1.146s, at 2018-11-14 13:31"
    } ]
  }, {
    "metadata" : {
      "id" : "3BE1959DCB9E4F5B861EA74F5B31DBD1"
    },
    "cell_type" : "markdown",
    "source" : "## Start Structured Streaming Continuous Processing using Spark Streaming\n"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "255A91A2EA3249B0B69D5CC7AD632818"
    },
    "cell_type" : "code",
    "source" : [ "import org.apache.spark.sql.functions._\n", "modelStream.foreachRDD{ rdd =>\n", "  if (!rdd.isEmpty) {\n", "    // Extract the new model parameters\n", "    val models = rdd.map{case (k,v) => v}.toDF(\"value\")\n", "    val mostRecentM2JsonModel = models.select(from_json($\"value\", idM2Schema) as \"record\")\n", "    val mostRecentM2Model = mostRecentM2JsonModel.select(\"record.*\").as[IdM2]\n", "    val m2Map = mostRecentM2Model.collect.map(idM2=> (idM2.id, idM2.m2)).toMap\n", "    \n", "    // Stop the continuous query, if running\n", "    if (query != null) {\n", "      query.stop()\n", "    }\n", "    \n", "    // Configure the scoring query with the new model parameters\n", "    val rawData = sparkSession.readStream\n", "      .format(\"kafka\")\n", "      .option(\"kafka.bootstrap.servers\", kafkaBootstrapServer)\n", "      .option(\"subscribe\", topic)\n", "      //.option(\"checkpointDir\", \"/tmp/model/checkpoint\")\n", "      .option(\"startingOffsets\", \"earliest\")\n", "      .load()\n", "    val rawValues = rawData.selectExpr(\"CAST(value AS STRING)\").as[String]\n", "    val jsonValues = rawValues.select(from_json($\"value\", sensorSchema) as \"record\")\n", "    val sensorData = jsonValues.select(\"record.*\").as[SensorData]\n", "    val scoreStream = sensorData.flatMap{case SensorData(id, ts, temp, hum) => \n", "                                     val m2Opt = m2Map.get(id)\n", "                                     m2Opt.map{m2 => (id, ts, temp, m2.mean, m2.stdev)}\n", "                                    }.toDF(\"id\", \"ts\",\"temp\",\"mean\",\"std\")\n", "    \n", "    // Apply the stdev model\n", "    val anomalies = scoreStream.where($\"temp\" > $\"mean\"+$\"std\"*threshold)\n", "    .select($\"id\" as \"key\", to_json(struct($\"id\",$\"ts\", $\"temp\")) as \"value\" )\n", "    import org.apache.spark.sql.streaming.Trigger\n", "    \n", "    // write the data back to Kafka\n", "    query = anomalies.writeStream\n", "      .format(\"kafka\")\n", "      .queryName(\"continuousStreamDetection\")\n", "      .trigger(Trigger.Continuous(\"10 second\"))\n", "      .outputMode(\"append\") \n", "      .option(\"kafka.bootstrap.servers\", kafkaBootstrapServer)\n", "      .option(\"topic\", anomalyTopic)\n", "      .option(\"checkpointLocation\", \"/tmp/spark/checkpoint-a1\")\n", "      .option(\"failOnDataLoss\", \"false\")\n", "    .start()\n", "  }\n", "}\n", "\n", "    " ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import org.apache.spark.sql.functions._\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 8,
      "time" : "Took: 2.636s, at 2018-11-14 13:31"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "D329053A96304C78B525007B9F6D5456"
    },
    "cell_type" : "code",
    "source" : [ "streamingContext.start()" ],
    "outputs" : [ {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 9,
      "time" : "Took: 1.523s, at 2018-11-14 13:31"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "B955BA0A45FF402E9F653A91DC857AEA"
    },
    "cell_type" : "code",
    "source" : [ "//streamingContext.stop(false)" ],
    "outputs" : [ {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 10,
      "time" : "Took: 1.191s, at 2018-11-14 13:26"
    } ]
  }, {
    "metadata" : {
      "id" : "BBB21A0011E14F7F89B017D8BAAD0CC4"
    },
    "cell_type" : "markdown",
    "source" : "## Read the Anomaly Stream Back From Kafka"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "D681695818F146B79C827C8081A6ED4D"
    },
    "cell_type" : "code",
    "source" : [ "val anomalyDataStream = sparkSession.readStream\n", "      .format(\"kafka\")\n", "      .option(\"kafka.bootstrap.servers\", kafkaBootstrapServer)\n", "      .option(\"subscribe\", anomalyTopic)\n", "      .option(\"startingOffsets\", \"latest\")\n", "      .load()" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "anomalyDataStream: org.apache.spark.sql.DataFrame = [key: binary, value: binary ... 5 more fields]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 10,
      "time" : "Took: 2.770s, at 2018-11-14 13:31"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "9B15466240D1494C871CE30606A8CBA5"
    },
    "cell_type" : "code",
    "source" : [ "case class AnomalyReport(id: String, ts: Long, temp: Double)\n", "import org.apache.spark.sql.Encoders\n", "val schema = Encoders.product[AnomalyReport].schema" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "defined class AnomalyReport\nimport org.apache.spark.sql.Encoders\nschema: org.apache.spark.sql.types.StructType = StructType(StructField(id,StringType,true), StructField(ts,LongType,false), StructField(temp,DoubleType,false))\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 11,
      "time" : "Took: 1.030s, at 2018-11-14 13:31"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "39E1CF9668D343819F3C979874A6B2D8"
    },
    "cell_type" : "code",
    "source" : [ "val rawValues = anomalyDataStream.selectExpr(\"CAST(value AS STRING)\").as[String]\n", "val jsonValues = rawValues.select(from_json($\"value\", schema) as \"record\")\n", "val anomalyData = jsonValues.select(\"record.*\").as[AnomalyReport]" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "rawValues: org.apache.spark.sql.Dataset[String] = [value: string]\njsonValues: org.apache.spark.sql.DataFrame = [record: struct<id: string, ts: bigint ... 1 more field>]\nanomalyData: org.apache.spark.sql.Dataset[AnomalyReport] = [id: string, ts: bigint ... 1 more field]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 12,
      "time" : "Took: 1.462s, at 2018-11-14 13:31"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "8F003E3A450F43958C7E708AFC9B13D1"
    },
    "cell_type" : "code",
    "source" : [ "import org.apache.spark.sql.types._\n", "val toSeconds = udf((ts:Long) => ts/1000)\n", "val anomalyReport = anomalyData.withColumn(\"timestamp\", toSeconds($\"ts\").cast(TimestampType))\n", "                                          .withWatermark(\"timestamp\", \"0 second\")\n", "                                          .groupBy($\"id\", window($\"timestamp\", \"1 minute\", \"1 second\"))\n", "                                          .agg(count($\"id\") as \"count\", max($\"temp\") as \"temp\")\n", "                                          " ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import org.apache.spark.sql.types._\ntoSeconds: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,LongType,Some(List(LongType)))\nanomalyReport: org.apache.spark.sql.DataFrame = [id: string, window: struct<start: timestamp, end: timestamp> ... 2 more fields]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 13,
      "time" : "Took: 2.377s, at 2018-11-14 13:31"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "348B80B01545467A9B090C47D372091C"
    },
    "cell_type" : "code",
    "source" : [ "val memQuery = anomalyReport.writeStream\n", "           .format(\"memory\")\n", "           .queryName(\"anomalyMemReport\")\n", "           .outputMode(\"append\")\n", "           .start()" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "memQuery: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@68468075\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 15,
      "time" : "Took: 1.802s, at 2018-11-14 13:32"
    } ]
  }, {
    "metadata" : {
      "id" : "4CC61893D0C44D1FAAA3BCFF019426E3"
    },
    "cell_type" : "markdown",
    "source" : "## Anomalies Chart"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "E061E8E7F28441BDB9C5935F16B48FBC"
    },
    "cell_type" : "code",
    "source" : [ "case class Bubble(id: String, count: Int, temp: Double, pos: Int = 0, color: String = \"red\")" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "defined class Bubble\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 16,
      "time" : "Took: 0.845s, at 2018-11-14 13:32"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "134B52484B6F40F086E020A5D475602C"
    },
    "cell_type" : "code",
    "source" : [ "val bubbles = Seq(Bubble(\"zero\",0, 1, 0, \"black\"), Bubble(\"zero\",1, 1000, 100, \"black\")))\n", "val bubbleChart = CustomPlotlyChart(bubbles, \n", "                  layout=\"{title: 'Anomaly Board', showlegend: false, height: 1000, width: 1000}\",\n", "                  dataOptions=\"{mode: 'markers'}\",\n", "                  dataSources=\"{x: 'pos', y: 'temp',text: 'id', marker: {size: 'count', color: 'color'}}\")" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "bubbles: Seq[Bubble] = List(Bubble(zero,0,1.0,0,black))\nbubbleChart: notebook.front.widgets.charts.CustomPlotlyChart[Seq[Bubble]] = <CustomPlotlyChart widget>\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 20,
      "time" : "Took: 1.338s, at 2018-11-14 13:33"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "C2AB56A90497432E8CA7F54643BC64E3"
    },
    "cell_type" : "code",
    "source" : [ "@volatile var running = true" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "running: Boolean = true\n"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "71C2F44DC9184C0BA3B7C6AE6631AC86"
    },
    "cell_type" : "code",
    "source" : [ "bubbleChart" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res26: notebook.front.widgets.charts.CustomPlotlyChart[Seq[Bubble]] = <CustomPlotlyChart widget>\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "<div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anon3a019b741250efd56f9a8aee44b31bc9&quot;,&quot;dataInit&quot;:[{&quot;count&quot;:0,&quot;pos&quot;:0,&quot;color&quot;:&quot;black&quot;,&quot;id&quot;:&quot;zero&quot;,&quot;temp&quot;:1}],&quot;genId&quot;:&quot;745500327&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/customPlotlyChart'], \n      function(playground, _magiccustomPlotlyChart) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magiccustomPlotlyChart,\n    \"o\": {\"js\":\"var layout = {title: 'Anomaly Board', showlegend: false, height: 600, width: 1000}; var dataSources={x: 'pos', y: 'temp',text: 'id', marker: {size: 'count', color: 'color'}}; var dataOptions = {mode: 'markers'}; var extraOptions = {}\",\"headers\":[\"id\",\"count\",\"temp\",\"pos\",\"color\"],\"height\":400}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    <div>\n      <span class=\"chart-total-item-count\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anone3f968053651df3abc2d52e040f187ec&quot;,&quot;initialValue&quot;:&quot;1&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p> entries total</span>\n      <span class=\"chart-sampling-warning\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anonab68c63516a573cc4964834ba4d9a5dd&quot;,&quot;initialValue&quot;:&quot;&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p></span>\n      <div>\n      </div>\n    </div></div>"
      },
      "output_type" : "execute_result",
      "execution_count" : 22,
      "time" : "Took: 1.429s, at 2018-11-14 13:33"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "2370A21BFEB449AA8E179F1711F65F7A"
    },
    "cell_type" : "code",
    "source" : [ "import scala.concurrent.duration._\n", "import scala.annotation.tailrec\n", "\n", "val updater = new Thread() {\n", "  @tailrec\n", "  def visualize(): Unit = {\n", "    val data = sparkSession.sql(s\"select * from anomalyMemReport\")\n", "    val indexedData = data.withColumn(\"pos\", lit(1)).withColumn(\"color\", lit(\"red\")).orderBy($\"window.start\".desc)\n", "                          .withColumn(\"count\", $\"count\".cast(IntegerType))\n", "    val bubbleData = indexedData.as[Bubble].take(50).sortBy(_.id).zipWithIndex\n", "        .map{case (bubble,idx) => bubble.copy(pos=idx, count = (bubble.temp * (1+bubble.count/10)).toInt)}\n", "    val filteredBubbleData = bubbleData.groupBy(_.id).mapValues{bubbles => bubbles.sortBy(b => -b.count).head}.values.toList\n", "    \n", "    if (filteredBubbleData.nonEmpty) bubbleChart.applyOn(filteredBubbleData)\n", "    if (running) {\n", "      Thread.sleep(1.second.toMillis)\n", "      visualize()\n", "    } else ()\n", "  } \n", "  \n", "  override def run() {\n", "    visualize()\n", "  }\n", "}.start()\n" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "D38B852520F944CE819CF0B23F85BD80"
    },
    "cell_type" : "code",
    "source" : [ "--" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "949C3CC7C314434984FCF83588819EE5"
    },
    "cell_type" : "code",
    "source" : [ "// execute to stop the chart updating thread\n", "running = false" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "running: Boolean = false\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 32,
      "time" : "Took: 1.158s, at 2018-11-14 13:40"
    } ]
  }, {
    "metadata" : {
      "id" : "73CC98AACACD41848CC0F78478C5C03B"
    },
    "cell_type" : "markdown",
    "source" : "# -- o --"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab2113607371-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "76A730A3B6DA45B58D350EBE86C260EF"
    },
    "cell_type" : "code",
    "source" : [ "memTable.where($\"id\" === \"office\")" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res150: org.apache.spark.sql.Dataset[(String, Long, Double)] = [id: string, ts: bigint ... 1 more field]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "<div class=\"df-canvas\">\n      <script data-this=\"{&quot;dataId&quot;:&quot;anon647819b1833202a2f70aeb0d3396743a&quot;,&quot;partitionIndexId&quot;:&quot;anon7291e2f670ae487b2e46f91e98d62005&quot;,&quot;numPartitions&quot;:9,&quot;dfSchema&quot;:{&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[{&quot;name&quot;:&quot;id&quot;,&quot;type&quot;:&quot;string&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;ts&quot;,&quot;type&quot;:&quot;long&quot;,&quot;nullable&quot;:false,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;temp&quot;,&quot;type&quot;:&quot;double&quot;,&quot;nullable&quot;:false,&quot;metadata&quot;:{}}]}}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/dataframe','../javascripts/notebook/consoleDir'], \n      function(dataframe, extension) {\n        dataframe.call(data, this, extension);\n      }\n    );/*]]>*/</script>\n      <link rel=\"stylesheet\" href=\"/assets/stylesheets/ipython/css/dataframe.css\" type=\"text/css\"/>\n    </div>"
      },
      "output_type" : "execute_result",
      "execution_count" : 93,
      "time" : "Took: 1.653s, at 2018-09-12 00:40"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "8C79874D846F4EA8849475E6C16BA620"
    },
    "cell_type" : "code",
    "source" : [ "" ],
    "outputs" : [ ]
  } ],
  "nbformat" : 4
}